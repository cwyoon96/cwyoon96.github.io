I"Ò(<h3 id="645-gaussian-processes-for-classification">6.4.5 Gaussian processes for classification</h3>

<p>ì´ë²ˆ ì ˆì—ì„œëŠ” Gaussian processë¥¼ ì´ìš©í•˜ì—¬ classification ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì„ ì†Œê°œí•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Gaussian processëŠ” real axis ì „ì²´ì—ì„œ predictionì„ í•˜ê¸° ë•Œë¬¸ì— sigmoid function $\sigma(x)$ì„ ì´ìš©í•´ probabilityë¥¼ predict í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼í•œë‹¤. Function $a(\mathbf{x})$ì— ëŒ€í•œ Gaussian processë¥¼ êµ¬í–ˆë‹¤ê³  ê°€ì •í•˜ë©´, target variable $t$ì˜ ë¶„í¬ëŠ” Bernoulli distribution</p>

\[p(t|a) = \sigma(a)^{t}(1 - \sigma(a))^{1-t}\]

<p>ë¡œ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤.</p>

<p>ë¨¼ì € $\mathbf{a}_{N+1}$ì— ëŒ€í•œ Gaussian process priorëŠ”</p>

\[p(\mathbf{a}_{N+1}) = \mathcal{N}(\mathbf{a}_{N+1}|\mathbf{0},\mathbf{C}_{N+1})\]

<p>ë¡œ ì •ì˜ëœë‹¤. í•˜ì§€ë§Œ regression caseì™€ëŠ” ë‹¤ë¥´ê²Œ covariance matrixì— noiseê°€ ì¶”ê°€ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ (ëª¨ë“  labelì´ correctí•˜ë‹¤ê³  ê°€ì •). í•˜ì§€ë§Œ, covariance matrixê°€ positive definiteì„ì„ í™•ì‹¤ì‹œí•˜ê¸° ìœ„í•´ noiseì™€ ìœ ì‚¬í•œ term $\nu$ë¥¼ ì¶”ê°€í•´</p>

\[C(\mathbf{x}_n,\mathbf{x}_m) = k(\mathbf{x}_n,\mathbf{x}_m) + \nu \delta_{nm}\]

<p>ìœ¼ë¡œ covariance matrix $\mathbf{C}_{N+1}$ì„ ì •ì˜í•œë‹¤.</p>

<p>ì´ì œ, predictive distributionì€</p>

\[p(t_{N+1} = 1|\mathbf{t}_N) = \int p(t_{N+1} = 1|a_{N+1})p(a_{N+1}|\mathbf{t}_{N})da_{N+1}\]

<p>where</p>

\[p(t_{N+1} = 1|a_{N+1}) = \sigma(a_{N+1})\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤. í•˜ì§€ë§Œ ì´ ì ë¶„ì€ intractable í•˜ê¸° ë•Œë¬¸ì— posterior distribution 
$p(a_{N+1}|\mathbf{t}_N)$
ì— ëŒ€í•œ Gaussian approximationì„ ì§„í–‰í•´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ <em>variational inference</em>, <em>expectation propagation</em> ë“±ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” Laplace approximationì„ í†µí•œ ë°©ë²•ì„ ì•Œì•„ë³´ì.</p>

<h3 id="646-laplace-approximation">6.4.6 Laplace approximation</h3>

<p>Bayesâ€™ theoremì„ í†µí•´ $a_{N+1}$ì— ëŒ€í•œ posterior distributionì€</p>

\[\begin{aligned}
p(a_{N+1}|\mathbf{t}_N) &amp;= \int p(a_{N+1},\mathbf{a}_N | \mathbf{t}_N)d\mathbf{a}_{N} \\ &amp;= \cfrac{1}{p(\mathbf{t}_N)} \int p(a_{N+1},\mathbf{a}_N)p(\mathbf{t}_N | a_{N+1},\mathbf{a}_N)d\mathbf{a}_{N} \\ &amp;= \cfrac{1}{p(\mathbf{t}_N)} \int p(a_{N+1}| \mathbf{a}_N)p(\mathbf{a}_N)p(\mathbf{t}_N | \mathbf{a}_N)d\mathbf{a}_N \\ &amp; = \int p(a_{N+1}| \mathbf{a}_N)p(\mathbf{a}_N|\mathbf{t}_N)d\mathbf{a}_N \ \ \ - (*)
\end{aligned}\]

<p>ë¡œ ì •ë¦¬ëœë‹¤. 2ë²ˆì§¸ì—ì„œ 3ë²ˆì§¸ ì¤„ìœ¼ë¡œ ë„˜ì–´ê°ˆ ë•ŒëŠ”</p>

\[p(\mathbf{t}_N|a_{N+1},\mathbf{a}_N) = p(\mathbf{t}_N|\mathbf{a}_N)\]

<p>ì„ì„ ì‚¬ìš©í–ˆë‹¤. ë˜í•œ, regression caseì—ì„œ ì •ë¦¬í•œ ê²°ê³¼ë¥¼ ì´ìš©í•´ conditional distribution 
$p(a_{N+1}|\mathbf{a}_N)$
ì€</p>

\[p(a_{N+1}|\mathbf{a}_N) = \mathcal{N}(a_{N+1}|\mathbf{k}^{T}\mathbf{C}_{N}^{-1}\mathbf{a}_N, c - \mathbf{k}^{T}\mathbf{C}_{N}^{-1}\mathbf{k})\]

<p>ë¡œ ì£¼ì–´ì§„ë‹¤. ë”°ë¼ì„œ posterior distribution 
<span>$p(\mathbf{a}_N|\mathbf{t}_N)$</span>
ì— ëŒ€í•œ Laplace approximationì„ êµ¬í•˜ë©´ (*)ì‹ì— ë”°ë¼ ì ë¶„ì„ í•˜ì—¬ 
<span>$a_{N+1}$</span>
ì— ëŒ€í•œ posterior distributionì„ êµ¬í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>Prior $p(\mathbf{a}_N)$ëŠ” zero meanì— covariance matrix $\mathbf{C}_N$ìœ¼ë¡œ ì£¼ì–´ì§€ë©°, data termì€</p>

\[p(\mathbf{t}_N | \mathbf{a}_N) = \prod_{n = 1}^{N}\sigma(a_n)^{t_n}(1-\sigma(a_n))^{1-t_n} = \prod_{n=1}^{N}e^{a_n t_n}\sigma(-a_n)\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤. ì´ì œ, additive normalization constant 
$p(\mathbf{t}_N)$
ë¥¼ ì œì™¸í•œ 
$p(\mathbf{a}_N|\mathbf{t}_N)$
ì˜ logarithmì— ëŒ€í•œ Taylor exansionì„ í•˜ë©´</p>

\[\begin{aligned}
\Psi(\mathbf{a}_N) &amp;= ln \ p(\mathbf{a}_N) + ln \ p(\mathbf{t}_N|\mathbf{a}_N) \\ &amp;= -\cfrac{1}{2}\mathbf{a}_{N}^{T}\mathbf{C}_{N}^{-1}\mathbf{a}_N - \cfrac{N}{2}ln(2\pi) - \cfrac{1}{2}ln |\mathbf{C}_N| + \mathbf{t}_{N}^{T}\mathbf{a}_N \\ &amp;-\sum_{n=1}^{N}ln(1 + e^{a_n}) + const 
\end{aligned}\]

<p>ë¡œ ì£¼ì–´ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ë¨¼ì €, posterior distributionì˜ mode ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ graidentë¥¼ ê³„ì‚°í•˜ë©´</p>

\[\bigtriangledown \Psi(\mathbf{a}_N) = \mathbf{t}_N - \boldsymbol{\sigma}_N - \mathbf{C}_{N}^{-1}\mathbf{a}_N\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ”ë°, $\sigma(a_n)$ì˜ ë²¡í„° í˜•íƒœì¸ $\boldsymbol{\sigma}_N$ê°€ $\mathbf{a}_N$ì— nonlinearí•˜ê²Œ dependí•˜ê¸° ë•Œë¬¸ì— ë‹¨ìˆœíˆ gradientë¥¼ 0ìœ¼ë¡œ ë‘ëŠ” ê²ƒ ë§Œìœ¼ë¡œëŠ” modeë¥¼ êµ¬í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ Newton-Raphson ë°©ë²•ê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì¨ì•¼í•˜ëŠ”ë° ì´ë¥¼ ìœ„í•´ second derivative</p>

\[\bigtriangledown \bigtriangledown \Psi(\mathbf{a}_N) = -\mathbf{W}_N - \mathbf{C}_{N}^{-1}\]

<p>where $\mathbf{W}_N$ is a diagonal matrix with elements $\sigma(a_n)(1-\sigma(a_n))$</p>

<p>ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ì´ë•Œ, diagonal elementê°€ (0,1/4) ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— $\mathbf{W}_N$ì€ poistive definite ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ $\mathbf{C}_N$ë„ p.d. ì´ê¸° ë•Œë¬¸ì— (ì—­í–‰ë ¬ë„ p.d.), ê·¸ í•© ë˜í•œ p.d. ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Hessian matrix $A = - \bigtriangledown \bigtriangledown \Psi(\mathbf{a}_N)$ê°€ p.d.ì„ì„ ì•Œ ìˆ˜ ìˆê³  posterior distributionì´ log convexí•˜ì—¬ ìœ ì¼í•œ global maximaë¥¼ ê°€ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, Hessianì´ $\mathbf{a}_N$ì— ëŒ€í•œ functionìœ¼ë¡œ ì£¼ì–´ì§€ê¸°ì— posteriorëŠ” ì—¬ì „íˆ Gaussianì€ ì•„ë‹ˆë‹¤.</p>

<p>ì´ì œ, 4ì¥ì—ì„œ ë‹¤ë¤˜ë˜ Newton-Raphsonn formulaë¥¼ ì´ìš©í•œ $\mathbf{a}_N$ì— ëŒ€í•œ iterative update ì‹ì€</p>

\[\mathbf{a}_{N}^{new} = \mathbf{C}_N(\mathbf{I} + \mathbf{W}_N \mathbf{C}_N)^{-1}\{\mathbf{t}_N - \boldsymbol{\sigma}_N + \mathbf{W}_N \mathbf{a}_N \}\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤. Mode ê°’ $\mathbf{a}_N^{*}$ì—ì„œëŠ” gradient ê°’ì€ 0ì´ ë  ê²ƒì´ê¸° ë•Œë¬¸ì—</p>

\[\mathbf{a}_N^{*} = \mathbf{C}_N (\mathbf{t}_N - \boldsymbol{\sigma}_N)\]

<p>ì‹ì´ ì„±ë¦½í•  ê²ƒì´ë‹¤. ì´ì œ $\mathbf{a}_N^{*}$ì—ì„œì˜ Hessian matrix $H$ë¥¼ ê³„ì‚°í•˜ë©´ posterior distributionì— ëŒ€í•œ Gaussian approximationì€</p>

\[q(\mathbf{a}_N) = \mathcal{N}(\mathbf{a}_N | \mathbf{a}_N^{*},H^{-1})\]

<p>ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

<p>ì´ì œ, ì›ë˜ êµ¬í•˜ê³ ì í•˜ì˜€ë˜ 
$p(a_{N+1}|\mathbf{t}_N)$
ì€ linear-Gaussian modelì— ë”°ë¼</p>

\[\begin{aligned}
\mathbb{E}[a_{N+1}|\mathbf{t}_N] &amp;= \mathbf{k}^{T}(\mathbf{t}_N - \boldsymbol{\sigma}_N) \\ \text{var}[a_{N+1}|\mathbf{t}_N] &amp;= c - \mathbf{k}^{T}(\mathbf{W}_N^{-1} + \mathbf{C}_N)^{-1}\mathbf{k}
\end{aligned}\]

<p>ì˜ ê°’ì„ ê°€ì§€ëŠ” Gaussian distributionìœ¼ë¡œ approximate í•  ìˆ˜ ìˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ predictive distributionì€ 6.4.5ì ˆì—ì„œ ì£¼ì–´ì§„ ì ë¶„ì‹ì„ í†µí•´ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

<p>Regression caseì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ classification caseì—ì„œë„ covariance functionì— ë“¤ì–´ê°„ parameter $\boldsymbol{\theta}$ë¥¼ ìµœì í™”í•˜ëŠ” ì‘ì—…ì„ í•´ì•¼í•œë‹¤. Likelihood functionì€</p>

\[p(\mathbf{t}_N|\boldsymbol{\theta}) = \int p(\mathbf{t}_N|\mathbf{a}_N)p(\mathbf{a}_N|\boldsymbol{\theta})d\mathbf{a}_N\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ”ë°, ì—­ì‹œ ì ë¶„ì´ intractableí•˜ê¸° ë•Œë¬¸ì— ë‹¤ì‹œ Laplace approximationì„ ì‚¬ìš©í•œë‹¤. 4.4ì ˆì—ì„œ ì •ë¦¬í•œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì´ìš©í•˜ì—¬</p>

\[ln \ p(\mathbf{t}_N|\boldsymbol{\theta}) = \Psi(\mathbf{a}_N^{*}) - \cfrac{1}{2}ln|\mathbf{W}_N + \mathbf{C}_{N}^{-1}| + \cfrac{N}{2}ln(2\pi)\]

<p>where</p>

\[\Psi(\mathbf{a}_N^{*}) = ln \ p(\mathbf{a}_N^{*}|\boldsymbol{\theta}) + ln \ p(\mathbf{t}_N|\mathbf{a}_N^{*})\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤. ì´ì œ gradientë¥¼ êµ¬í•˜ë©´ ë˜ëŠ”ë°, covariance matrix $\mathbf{C}_N$ì˜ $\boldsymbol{\theta}$ì— ëŒ€í•œ explicití•œ dependenceë¡œ ì¸í•œ termê³¼ $\mathbf{a}_N^{*}$ì˜ $\boldsymbol{\theta}$ì— ëŒ€í•œ dependenceë¡œ ì¸í•œ term 2ê°€ì§€ë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë¨¼ì €, explicit dependence termì€</p>

\[\begin{aligned}
\cfrac{\partial ln \ p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial \theta_j} \ = \ &amp;\cfrac{1}{2}\mathbf{a}_{N}^{*T}\mathbf{C}_{N}^{-1}\cfrac{\partial \mathbf{C}_N}{\partial \theta_j}\mathbf{C}_{N}^{-1}\mathbf{a}_{N}^{*} \\ &amp;- \cfrac{1}{2}Tr\bigg[ (\mathbf{I} + \mathbf{C}_N\mathbf{W}_N)^{-1}\mathbf{W}_N\cfrac{\partial \mathbf{C}_N}{\partial \theta_j} \bigg]
\end{aligned}\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤.</p>

<p><span>$\mathbf{a}_N^{*}$</span>
ì˜ parameterì— ëŒ€í•œ dependenceë¡œ ì¸í•œ termì—ì„œ 
<span>$\Psi(\mathbf{a}_N)$</span>
ëŠ” 
<span>$\mathbf{a}_N^{*}$</span>
ì—ì„œ gradientê°€ 0ì´ ë˜ë„ë¡ Laplace approximationì„ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì—, 2ë²ˆì§¸ termì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ë©´ ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 2ë²ˆì§¸ termì— ëŒ€í•œ gradientëŠ”</p>

\[\begin{aligned}
-\cfrac{1}{2}&amp;\sum_{n = 1}^{N} \cfrac{\partial ln|\mathbf{W}_N + \mathbf{C}_N^{-1}|}{\partial a_{n}^{*}}\cfrac{\partial a_n^*}{\partial \theta_j} \\ &amp;= -\cfrac{1}{2}\sum_{n=1}^{N}\big[ (\mathbf{I}+ \mathbf{C}_N\mathbf{W}_N)^{-1}\mathbf{C}_N \big]_{nn}\sigma_{n}^{*}(1 - \sigma_{n}^{*})(1-2\sigma_{n}^{*})\cfrac{\partial a_{n}^{*}}{\partial \theta_j}
\end{aligned}\]

<p>ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, ìœ„ì— ì£¼ì–´ì§„ $\mathbf{a}_N^{*}$ì— ëŒ€í•œ ì‹ì„ ì´ìš©í•˜ì—¬</p>

\[\cfrac{\partial a_{n}^{*}}{\partial \theta_j} = (\mathbf{I} + \mathbf{W}_N \mathbf{C}_N)^{-1}\cfrac{\partial \mathbf{C}_N}{\partial \theta_j}(\mathbf{t}_N - \boldsymbol{\sigma}_N)\]

<p>ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ì œ í•„ìš”í•œ ëª¨ë“  termë“¤ì„ êµ¬í–ˆìœ¼ë¯€ë¡œ graidentë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê³ , nonlinear optimization algorithmì„ ì´ìš©í•´ ìµœì ì˜ $\boldsymbol{\theta}$ë¥¼ êµ¬í•´ì£¼ë©´ ëœë‹¤.</p>

<h3 id="647-connection-to-neural-networks">6.4.7 Connection to neural networks</h3>

<p>Neural network íŒŒíŠ¸ì—ì„œ hidden unitì˜ ê°œìˆ˜ Mì„ ì¶©ë¶„íˆ ëŠ˜ë¦¬ë©´ two-layer networkê°€ ì–´ë–¤ functionì´ë“  approximateí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°°ì› ì—ˆë‹¤. Neal (1996)ì€ Bayesian neural networkì—ì„œ $\mathbf{w}$ì— ëŒ€í•œ íŠ¹ì • prior distributionë“¤ì„ ê°€ì •í•˜ì˜€ì„ ë•Œ, neural networkì—ì„œ ìƒì„±ëœ í•¨ìˆ˜ì˜ ë¶„í¬ê°€ Mì´ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§ì— ë”°ë¼ Gaussian processë¥¼ ë”°ë¥´ê²Œ ëœë‹¤ëŠ” ê²ƒì„ ë°í˜€ëƒˆë‹¤. í•˜ì§€ë§Œ, Mì´ ë¬´í•œëŒ€ë¡œ ê°€ë©´ outputì€ ì„œë¡œ indenpendent í•˜ê²Œ ë  ê²ƒì´ê³  ì´ë ‡ê²Œ ë˜ë©´ ê°™ì€ hidden unitì„ ê³µìœ í•œë‹¤ëŠ” neural networkì˜ ì¥ì ì„ ì‚´ë¦¬ì§€ ëª»í•œë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.</p>

<p>ë˜í•œ, Gaussian processëŠ” covariance function (kernel function)ì„ ì–´ë–»ê²Œ ì •ì˜í•˜ëƒì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ í™•ì¸í–ˆëŠ”ë°, Willianms (1998)ëŠ” hidden unit activation functionì„ probit í˜¹ì€ Gaussianìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ì§€ì— ë”°ë¥¸ covariance functionì˜ explicit formì„ ì œì‹œí•˜ì˜€ë‹¤.</p>
:ET