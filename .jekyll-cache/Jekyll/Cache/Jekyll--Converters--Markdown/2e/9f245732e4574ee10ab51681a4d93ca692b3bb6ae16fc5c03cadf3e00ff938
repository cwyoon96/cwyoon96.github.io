I"ó'<h2 id="43-probabilistic-discriminative-model">4.3 Probabilistic Discriminative Model</h2>

<p>4.2ì ˆì—ì„œëŠ” 
<span>$p(\mathbf{x}|C_k)$ </span>
ì— ëŒ€í•œ ë¶„í¬ ê°€ì •ì„ í†µí•´ Bayes Thmì„ ì´ìš©í•˜ì—¬ posterior 
<span>$p(C_k|\mathbf{x})$ </span>
ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í–ˆë‹¤. ê·¸ ê²°ê³¼ 
<span>$p(C_k|\mathbf{x})$</span>
ê°€ 
$\mathbf{x}$
ì— ëŒ€í•œ linear formìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. 4.3ì ˆì—ì„œëŠ” ê±°ê¸°ì„œ ì°©ì•ˆí•˜ì—¬, 
<span>$p(\mathbf{x}|C_k)$</span>
ì— ëŒ€í•œ ë¶„í¬ê°€ì • ì—†ì´ Generalized Linear Modelì„ ì´ìš©í•´ Probabilistic Discriminative Modelì„ ìƒì„±í•˜ê³  parameterë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•œë‹¤.</p>

<h3 id="431-fixed-basis-functions">4.3.1 Fixed basis functions</h3>

<p>4ì¥ ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ input vector $\mathbf{x}$ì˜ ê³µê°„ì—ì„œ classification modelë“¤ì„ ìƒê°í–ˆì§€ë§Œ, 3ì¥ì—ì„œ ë‹¤ë£¬ ê²ƒì²˜ëŸ¼ basis function $\phi(\mathbf{x})$ë¥¼ ì´ìš©í•˜ì—¬ non-linearí•œ transformì„ ê±°ì¹œ í›„ feature spaceì—ì„œ classification modelì„ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ original input spaceì—ì„œ linear classificationì´ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì œë„ feature spaceì—ì„œ linear í•˜ê²Œ í•´ê²°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— classificationì—ì„œë„ ë§¤ìš° ì¤‘ìš”í•œ ë°©ë²•ì´ë‹¤. ë¬¼ë¡ , ì´ëŸ¬í•œ fixed basis function ëª¨ë¸ì€ ë‚˜ë¦„ëŒ€ë¡œì˜ í•œê³„ì ë“¤ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì—, ì´í›„ chapterì—ì„œ basis functionì„ ë°ì´í„°ì— ë§ì¶° adaptí•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£° ê²ƒì´ë‹¤.</p>

<h3 id="432-logistic-regression">4.3.2 Logistic regression</h3>

<p>ë¨¼ì € two-class classification ë¬¸ì œë¥¼ ìƒê°í•´ë³´ì. 4.2ì ˆì—ì„œ continuous inputì˜ ê²½ìš° 
<span>$p(C_1|\mathbf{x}) = \sigma(\mathbf{w}^{T}\mathbf{x}+w_0)$</span>
ì˜ í˜•íƒœë¡œ ì£¼ì–´ì§€ëŠ” ê²ƒì„ ë³´ì˜€ê³ , 
<span>$\mathbf{w},w_0$</span>
ëŠ” Gaussian Parameterë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì£¼ì–´ì¡Œë‹¤. ì—¬ê¸°ì„œ ì°©ì•ˆí•˜ì—¬, ì„ì˜ì˜ parameter 
<span>$\mathbf{w}$</span>
ì— ëŒ€í•´</p>

\[p(C_1|\phi) = y(\phi) = \sigma(\mathbf{w}^T\phi)\]

<p>ì‹ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ì„ ìƒê°í•  ìˆ˜ ìˆëŠ”ë°, (ì—¬ê¸°ì„œ $\phi$ëŠ” 4.3.1ì—ì„œì˜ feature vector $\phi(\mathbf{x})$) ì´ê²ƒì´ Logistic Regression Model ì´ë‹¤.</p>

<p>Logistic regressionì˜ ì¥ì ì€ 4.2ì ˆì—ì„œ Gaussian class conditional densityë¥¼ í†µí•´ Generative Modelì„ ë§Œë“¤ì—ˆë˜ ê²ƒì— ë¹„í•´ ì¶”ì •í•´ì•¼í•  parameterì˜ ê°œìˆ˜ê°€ í›¨ì”¬ ì ë‹¤ëŠ” ê²ƒì´ë‹¤. Gaussian densityë¥¼ ì´ìš©í•  ê²½ìš° D-dimension featureì— ëŒ€í•´ $D(D+5)/2 + 1$ ê°œì˜ parameterë¥¼ ì¶”ì •í•´ì•¼ í•˜ì§€ë§Œ, Logistic regressionì˜ ê²½ìš° $\mathbf{w}$ ì•ˆì˜ $D$ê°œì˜ parameterë§Œ ì¶”ì •í•˜ë©´ ëœë‹¤.</p>

<p>ì´ì œ ë‹¤ì‹œ Maximum Likelihood Estimationì„ ì´ìš©í•´ parameterë¥¼ ì¶”ì •í•˜ë©´ ë˜ëŠ”ë°, likelihood functionì€</p>

\[p(\mathbf{t}|\mathbf{w}) = \prod_{n = 1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}\]

<p>ì˜ í˜•íƒœë¥¼ ê°€ì§€ë©° ì—¬ê¸°ì„œ $y_n = \sigma(\mathbf{w}^T\phi_n)$ì„ ì˜ë¯¸í•œë‹¤. Likelihood functionì— ë‹¤ì‹œ -log ë¥¼ ì·¨í•´ì„œ error functionì˜ í˜•íƒœë¡œ ë§Œë“¤ì–´ì£¼ë©´</p>

\[E(\mathbf{w}) = -\sum_{n = 1}^{N}\{t_nln\ y_n  + (1-t_n)ln \ (1-y_n) \}\]

<p>ì˜ í˜•íƒœê°€ ë˜ì–´ ê·¸ gradientëŠ”</p>

\[\bigtriangledown E(\mathbf{w}) = \sum_{n = 1}^{N}(y_n - t_n)\phi_n\]

<p>ì˜ í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤. ì´ëŠ” errorì— basis function vectorë¥¼ ê³±í•œ í˜•íƒœë¡œ, 3ì¥ì—ì„œ linear regression modelê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì£¼ì–´ì§„ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ, ì´ëŸ¬í•œ MLE ì¶”ì •ì€ linearly separable í•œ ë°ì´í„°ì—ì„œ ê·¹ì‹¬í•œ over-fitting ë¬¸ì œë¥¼ ë³´ì´ëŠ”ë°, ì´ëŸ° ê²½ìš° MAPë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ error functionì— regularizationì„ ì£¼ëŠ” ê²ƒì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="433-iterative-reweighted-least-squares">4.3.3 Iterative reweighted least squares</h3>

<p>Logistic regressionì˜ ê²½ìš° sigmoid functionì˜ non-linearityë¡œ ì¸í•´ MLEì˜ closed form solutionì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— Newton-Raphson iterative optimization</p>

\[\mathbf{w}^{new} = \mathbf{w}^{old} - H^{-1}\bigtriangledown E(\mathbf{w})\]

<p>ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë°, ì—¬ê¸°ì„œ HëŠ” $E(\mathbf{w})$ì˜ Hessian matrixì´ë‹¤. (ì°¸ê³ ë¡œ linear regression ë¬¸ì œì— Newton-Raphsonì„ ì‚¬ìš©í•˜ë©´ 1 stepì— ë¬¸ì œê°€ í’€ë¦°ë‹¤.) ì´ì œ í•´ë‹¹ ë°©ë²•ì„ logistic regressionì— ì ìš©í•˜ë©´,</p>

\[\begin{aligned}
\bigtriangledown E(\mathbf{w}) &amp;= \sum_{n=1}^{N}(y_n-t_n)\phi_n = \Phi^T(\mathbf{y}-\mathbf{t}) \\
H &amp;= \sum_{n=1}^{N}y_n(1-y_n)\phi_n\phi_n^T = \Phi^TR\Phi
\end{aligned}\]

<p>ì„ì„ ì´ìš©í•´ì„œ (ì—¬ê¸°ì„œ $R$ì€ $R_{nn} = y_n(1-y_n)$ë¥¼ ê°–ëŠ” diagonal matrixì´ë‹¤.)</p>

\[\mathbf{w}^{new} = (\Phi^TR\Phi)^{-1}\Phi^TR\mathbf{z}\]

<p>ì‹ì„ ì„¸ìš¸ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ $\mathbf{z}$ëŠ”</p>

\[\mathbf{z} = \Phi\mathbf{w}^{old} - R^{-1}(\mathbf{y} - \mathbf{t})\]

<p>ë¥¼ ì˜ë¯¸í•œë‹¤.ì‚¬ì‹¤ ìœ„ í˜•íƒœëŠ” weighted least squareì˜ í˜•íƒœì™€ ë™ì¼í•˜ë©° ì‹¤ì œë¡œ $R$ì€ $t$ì˜ variance matrixë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, $R$ì´ constantê°€ ì•„ë‹Œ $\mathbf{w}$ë¥¼ í¬í•¨í•˜ëŠ” matrixì´ê¸° ë•Œë¬¸ì— iterativeí•˜ê²Œ ê³„ì‚°ì„ í•´ì•¼í•˜ëŠ”ë°, ì´ë¡œ ì¸í•´ ì´ ì•Œê³ ë¦¬ì¦˜ì„ iterative reweighted least square (IRLS)ë¼ê³  ë¶€ë¥¸ë‹¤. ì°¸ê³ ë¡œ, $H$ê°€ positive definite í•˜ë‹¤ëŠ” íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ error functionì€ $\mathbf{w}$ì˜ concave í•¨ìˆ˜ê°€ ë˜ì–´ at most one unique solutionì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.</p>

<h3 id="434-multiclass-logistic-regression">4.3.4 Multiclass logistic regression</h3>

<p>ì´ëŸ¬í•œ logistic regressionì˜ ê°œë…ì„ 4.2ì ˆì—ì„œ ë‹¤ë¤˜ë˜ multiclassë¡œë„ í™•ì¥í•  ìˆ˜ ìˆëŠ”ë°</p>

\[p(C_k|\phi) = y_k(\phi) = \cfrac{exp(a_k)}{\sum_jexp(a_j)}\]

<p>where $a_k = \mathbf{w}_k^T\phi$ ë¡œ ë‘ê³  $\mathbf{w}_k$ì— ëŒ€í•œ maximum likelihoodë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•˜ë©´ ëœë‹¤.</p>

<p>Likelihood functionì€</p>

\[p(\mathbf{T}|\mathbf{w}_1,...,\mathbf{w}_k) = \prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}}\]

<p>ë¡œ ì£¼ì–´ì§€ë©°, ì—¬ê¸°ì„œ $y_{nk} = y_k(\phi_n)$ë¥¼ ì˜ë¯¸í•œë‹¤. ì´í›„ Logistic regressionê³¼ ë™ì¼í•œ ê³¼ì •ì„ ê±°ì³ Newton-Raphson methodë¥¼ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.</p>

<p>ì°¸ê³ ë¡œ, Multiclssì—ì„œë„ error functionì— ëŒ€í•œ gradientê°€</p>

\[\bigtriangledown_{\mathbf{w}_{j}}E(\mathbf{w_1},...,\mathbf{w}_k) = \sum_{n=1}^N(y_{nj} - t_{nj})\phi_n\]

<p>ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ errorì— feature vectorë¥¼ ê³±í•œ í˜•íƒœê°€ ë˜ëŠ”ë°, ì‚¬ì‹¤ ì´ëŠ” general resultë¡œ 4.3.6ì ˆì—ì„œ ì´ì— ëŒ€í•´ ë‹¤ë£° ê²ƒì´ë‹¤.</p>

<h3 id="435-probit-regression">4.3.5 Probit regression</h3>

<p>ë‹¤ì‹œ two-class ë¬¸ì œë¡œ ëŒì•„ì™€ì„œ sigmoid functionì´ ì•„ë‹Œ ë‹¤ë¥¸ activation fuctionì„ ì´ìš©í•˜ëŠ” ë” generalí•œ ì¼€ì´ìŠ¤ê°€ ìˆì„ ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³´ì. ì´ë¥¼ ìœ„í•´ noisy threshold modelì„ ìƒê°í•  ìˆ˜ ìˆëŠ”ë°, $a_n = \mathbf{w}^T\phi_n$ ì— ëŒ€í•´</p>

\[\begin{cases}
      t_n = 1 &amp; \text{if}\ a_n \geq \theta \\
      t_n = 0, &amp; \text{otherwise}
\end{cases}\]

<p>ì¸ ëª¨ë¸ì„ ìƒê°í•´ë³´ì. ì—¬ê¸°ì„œ $\theta$ë¥¼ probability density $p(\theta)$ì—ì„œ draw í•œë‹¤ê³  ìƒê°í•˜ë©´ activation functionì€</p>

\[f(a) = \int_{-\infty}^{a}p(\theta)d\theta\]

<p>ë¡œ ì£¼ì–´ì§ˆ ê²ƒì´ë©°, íŠ¹íˆ density $p(\theta)$ë¥¼ zero mean unit variance Gaussianì´ë¼ ê°€ì •í•˜ë©´ probit function $\Phi(a)$ë¥¼ activation functionìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ° ê²½ìš°ë¥¼ probit regressionì´ë¼ ë¶€ë¥¸ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ probit regressionì€ logistic regressionê³¼ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ ì´í›„ 4.5 ì ˆì—ì„œ Bayesian treatmentì™€ ê´€ë ¨í•˜ì—¬ probit modelì„ ë‹¤ë£° ê²ƒì´ë‹¤.</p>

<p>Probit regressionì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” logistic regressionì— ë¹„í•´ outlierì— ë” sensitive í•˜ë‹¤ëŠ” ê²ƒì´ì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ logistic regessionê³¼ probit regression ëª¨ë‘ dataê°€ ì˜¬ë°”ë¥´ê²Œ label ë˜ì—ˆë‹¤ê³  ê°€ì •í•œë‹¤. ë§Œì•½, mislabelingì— ëŒ€í•œ ë¶€ë¶„ì„ ìƒê°í•œë‹¤ë©´ mislabelì— ëŒ€í•œ í™•ë¥  $\epsilon$ì„ probabilistic modelì— incorporate í•˜ì—¬</p>

\[p(t|\mathbf{x}) = (1 - \epsilon)\sigma(\mathbf{x}) + \epsilon(1-\sigma(\mathbf{x}))\]

<p>ë¡œ ë‘ê³  ë¬¸ì œë¥¼ í’€ ìˆ˜ë„ ìˆë‹¤.</p>

<h3 id="436-canonical-link-functions">4.3.6 Canonical link functions</h3>

<p>ì•ì„œ linear regression, logistic regression ê·¸ë¦¬ê³  multiclass logistic regressionì—ì„œ ëª¨ë‘ gradientê°€ errorì— feature vectorë¥¼ ê³±í•œ í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ì´ë²ˆ ì ˆì—ì„œëŠ” ì´ resultê°€ íŠ¹ì • ì¡°ê±´ì—ì„œ general resultì„ì„ ë³´ì¼ ê²ƒì´ë‹¤.</p>

<p>ë¨¼ì € target variableì˜ conditional distributionì´ exponential family ì„ì„ ê°€ì •í•˜ì—¬</p>

\[p(t|\eta,s) = \cfrac{1}{s}h\bigg(\cfrac{t}{s}\bigg)g(\eta)exp\bigg\{\cfrac{\eta t}{s}\bigg\}\]

<p>ìœ¼ë¡œ ì“°ì. ê·¸ëŸ¬ë©´ $t$ì˜ conditional mean $y$ëŠ”</p>

\[y \equiv E(t|\eta) = -s\cfrac{d}{d\eta}ln \ g(\eta)\]

<p>ë¡œ ì£¼ì–´ì§„ë‹¤. ì´ëŸ¬í•œ $y$ì™€ $\eta$ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ ë°˜ëŒ€ë¡œ $\eta = \psi(y)$ë¥¼ ë§Œì¡±í•˜ëŠ” $\psi$ë¥¼ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë˜í•œ log likelihood functionì„ êµ¬í•˜ë©´</p>

\[ln \ p(\mathbf{t}|\eta,s) = \sum_{n = 1}^{N}ln \ p(t_n|\eta,s) = \sum_{n=1}^{N}\bigg\{ ln \ g(\eta_n) + \cfrac{\eta_n t_n}{s}\bigg\} + const\]

<p>ë¡œ ì£¼ì–´ì§„ë‹¤.</p>

<p>ì´ì œ ë‹¤ì‹œ generalized linear modelì„ ì •ì˜í•˜ì—¬ $y_n = f(a_n) =f(\mathbf{w}^T\phi_n)$ ë¡œ ë‘ì. ì—¬ê¸°ì„œ $f$ ëŠ” activation function, $f^{-1}$ëŠ” link functionìœ¼ë¡œ ì •ì˜ëœë‹¤. Log likelihood functionì„ $\mathbf{w}$ì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´</p>

\[\begin{aligned}
\bigtriangledown_{\mathbf{w}}ln \ p(\mathbf{t}|\eta,s) &amp;= \sum_{n = 1}^{N}\bigg\{\cfrac{d}{d\eta_n} \ ln \ g(\eta_n) + \cfrac{t_n}{s}\bigg\}\cfrac{d\eta_n}{dy_n}\cfrac{dy_n}{da_n}\bigtriangledown_{a_n} \\
&amp;= \sum_{n = 1}^{N}\cfrac{1}{s}\{t_n - y_n\}\psi'(y_n)f'(a_n)\phi_n
\end{aligned}\]

<p>ìœ¼ë¡œ gradientë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ, link functionì„</p>

\[f^{-1}(y) = \psi(y)\]

<p>ë¡œ ë‘ë©´ $f(\psi(y)) = y$ë¡œ ë¶€í„° $fâ€™(\psi)\psiâ€™(y) = 1$ ì„ì„ ì•Œ ìˆ˜ ìˆê³  (chain rule), $a_n = f^{-1}(y_n)$ì„ì„ ì´ìš©í•˜ì—¬ $a = \psi(y_n)$ ë¡œ ì£¼ì–´ì§€ê¸° ë•Œë¬¸ì— ìµœì¢…ì ìœ¼ë¡œ $fâ€™(a_n)\psiâ€™(y_n) = 1$ ë¡œ ì£¼ì–´ì§„ë‹¤. ë”°ë¼ì„œ graidientê°€</p>

\[\bigtriangledown_{\mathbf{w}}E(\mathbf{w}) =\sum_{n = 1}^{N}\cfrac{1}{s}\{t_n - y_n\}\phi_n\]

<p>ë¡œ ì£¼ì–´ì ¸ ìš°ë¦¬ê°€ ì•„ëŠ” errorì— feature vectorì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¨ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. ì´ë•Œ í•´ë‹¹ link functionì„ canonical link fuctionì´ë¼ ë¶€ë¥¸ë‹¤.</p>
:ET