<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>PRML 스터디 Chap.2 (2) | Statistics and Machine Learning Lab by cwyoon96</title>
<meta name="description" content="(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.)">


  <meta name="author" content="cwyoon96">
  
  <meta property="article:author" content="cwyoon96">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Statistics and Machine Learning Lab by cwyoon96">
<meta property="og:title" content="PRML 스터디 Chap.2 (2)">
<meta property="og:url" content="http://localhost:4000/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-2-2/">


  <meta property="og:description" content="(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.)">







  <meta property="article:published_time" content="2021-09-21T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-2-2/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "cwyoon96",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});

</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Statistics and Machine Learning Lab by cwyoon96 Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Statistics and Machine Learning Lab by cwyoon96
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">cwyoon96</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>통계학 그리고 머신러닝을 공부하는 사람입니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Daejeon, South Korea</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/cwyoon96" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="PRML 스터디 Chap.2 (2)">
    <meta itemprop="description" content="(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.)">
    <meta itemprop="datePublished" content="2021-09-21T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-2-2/" class="u-url" itemprop="url">PRML 스터디 Chap.2 (2)
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 분 소요
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu"><li><a href="#23-gaussian-distribution">2.3 Gaussian Distribution</a><ul><li><a href="#231-conditional-gaussian-distribution">2.3.1 Conditional Gaussian Distribution</a></li><li><a href="#232-marginal-gaussian-distribution">2.3.2 Marginal Gaussian Distribution</a></li><li><a href="#233-bayes-theorem-for-gaussian-variables">2.3.3 Bayes’ theorem for Gaussian variables</a></li><li><a href="#234-maximum-likelihood-for-the-gaussian">2.3.4 Maximum Likelihood for the Gaussian</a></li><li><a href="#235-sequential-estimation">2.3.5 Sequential estimation</a></li><li><a href="#236-bayesian-inference-for-the-gaussian">2.3.6 Bayesian Inference for the Gaussian</a></li><li><a href="#237-students-t-distribution">2.3.7 Student’s t-distribution</a></li><li><a href="#238-periodic-variables">2.3.8 Periodic variables</a></li><li><a href="#239-mixture-of-gaussians">2.3.9 Mixture of Gaussians</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <p>(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.)</p>

<p>이번에는 PRML 2.3절에 해당하는 Gaussian Distribution에 대한 내용이다. 정규분포 혹은 가우시안 분포로 불리는 Gaussian Distribution은 통계학 그리고 머신러닝에서 정말 중요한 위치를 가지는 분포인 만큼 책에도 많은 내용이 담겨져 있다. 특히, 책에는 수식을 통해 결과값을 유도하는 과정이 많이 담겨져 있는데, 원활한 진행을 위해 해당 부분은 생략하였으니 자세한 증명은 PRML 책 혹은 편한 수리통계학 책을 참고하자.</p>

<h2 id="23-gaussian-distribution">2.3 Gaussian Distribution</h2>

<p>Gaussian Distribution은</p>

\[N(x|u,\sigma^2) = \cfrac{1}{(2\pi\sigma^2)^{1/2}}exp\{-\cfrac{1}{2\sigma^2}(x-u)^2\}\]

<p>을 pdf로 가지는 분포로, Multivariate version pdf는</p>

\[N(\mathbf{x}|\mathbf{u},\Sigma) = \cfrac{1}{(2\pi)^{D/2}}\cfrac{1}{|\Sigma|^{1/2}}exp\{-\cfrac{1}{2}(\mathbf{x} - \mathbf{u})^T\Sigma^{-1}(\mathbf{x} - \mathbf{u})\}\]

<p>의 형태를 가진다. (차원의 수가 $D$라고할 때) Bell Shaped Distribution으로 고등학교 확률과 통계 교과서에도 등장하는 분포이다.</p>

<p>여기서 $\mathbf{u}$ 와 $\Sigma$ 는 각각 $E(\mathbf{x})$ 과 $Var(\mathbf{x})$ 를 의미하는 모수이며, $\Sigma$ 는 그 특성상 Symmetric 하다.</p>

<p>이러한 정규분포를 이용한 정리에서 가장 중요한 정리 중 하나라고 할 수 있는 <em>Central Limit Theorem</em> 은, 특정 조건 하에, 표본이 어떠한 분포를 따르는지와 상관 없이 표본의 수가 증가함에 따라 표본의 평균 (<strong>표본 자체가 아니다</strong>)이 정규분포를 따른다는 정리이다. 이 정리를 이용하여 많은 통계적 문제를 정규분포를 이용해서 풀 수 있다.</p>

<p>Multivariate Normal의 p.d.f.에서 Exponential 항에 들어가 있는 quadratic form</p>

\[\bigtriangleup^2 = (\mathbf{x}-\mathbf{u})^T\Sigma^{-1}(\mathbf{x}-\mathbf{u})\]

<p>에서 $\bigtriangleup$은 <em>Mahalanobis Distance</em>로 불리며, 만약 $\Sigma$가 $I$ 라면 Euclidean Distance와 같은 의미를 갖게 된다. 2차원에서 예를 들면, <em>Mahalanobis Distance</em>는 다음을 가능하게 해준다.</p>

<p><img src="/assets/img/2021-09-21-prml-스터디-chap-2-2/KakaoTalk_Photo_2021-09-26-13-49-01.jpeg" alt="" title="Mahalanobis Dist" /></p>

<p>해당 그림에서 Euclidean Distnace를 기준으로 했다면 원점에서 x까지의 거리와 y까지의 거리는 서로 달랐겠지만, Mahalanobis Distance를 기준으로 한다면 동일하다. 이러한 특성으로 인해 2차원 정규분포의 p.d.f. 가 타원형의 contour로 나타날 수 있게 된다.</p>

<p>또한, $\Sigma$의 Eigenvalue Decomposition과 Change of Coordinate를 이용하여 Multivariate Normal p.d.f. 를</p>

\[p(y) = \prod_{j=1}^{D}\cfrac{1}{(2\pi\lambda_j)^{1/2}}exp\{-\cfrac{y_j^2}{2\lambda_j}\}\]

<p>로 표현할 수 있다. 여기서 $\lambda_j$와 $y_j$는 각각 Eigenvalue와 PC score를 의미한다. 이는 Multivariate Normal이 D개의 독립적인 Univariate Normal로 표현할 수 있다는 것을 의미한다.</p>

<p>이러한 Gaussian Distribution의 가장 큰 단점은 바로, 그 특성상 Unimodal (Single Maximum)하기 때문에, Multimodal (Multiple Maximum)한 데이터를 표현하기 적합하지 않다는 것이다. 이러한 점을 해결하기 위해 이후 Gaussian Mixture Model을 배우게 될 것이다.</p>

<h3 id="231-conditional-gaussian-distribution">2.3.1 Conditional Gaussian Distribution</h3>

<p>Gaussian 분포의 대표적인 특성 중 하나는 Conditional Distribution과 Marginal Distribution이 모두 다시 Gaussian 분포를 따른다는 것이다. 먼저 Conditional Distribution을 살펴보면,</p>

\[\begin {aligned} \mathbf{x} &amp;= \begin{pmatrix} \mathbf{x}_{a} \\\\ \mathbf{x}_{b} \end{pmatrix} \\\\ \mathbf{u} &amp;= \begin{pmatrix} \mathbf{u}_{a} \\\\ \mathbf{u}_{b} \end{pmatrix} \\\\
\Sigma &amp;= \begin{pmatrix} \Sigma_{aa} \Sigma_{ab} \\\\ \Sigma_{ba} \Sigma_{bb} \end{pmatrix} \end {aligned}\]

<p>로 partition을 했을 때 Conditional Distribution 
$p(\mathbf{x}_a|\mathbf{x}_b)$
는</p>

\[\begin {aligned} \mathbf{u}_{a|b} &amp;= \mathbf{u}_{a} + \Sigma_{ab}\Sigma_{bb}^{-1}(\mathbf{x}_{b} - \mathbf{u}_{b}) \\\\ \Sigma_{a|b} &amp;= \Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba} \end {aligned}\]

<p>의 평균과 분산을 갖는 Gaussian Distribution을 따른다. 여기서 평균은 $\mathbf{x}_b$ 의 Linear Function으로 나타나고 분산은 $\mathbf{x}_a$ 와 무관함을 알 수 있는데, 이는 추후에 다룰 <em>Linear-Gaussian Model</em> 의 예시 중 하나로 볼 수 있다.</p>

<h3 id="232-marginal-gaussian-distribution">2.3.2 Marginal Gaussian Distribution</h3>

<p>위에서 언급했듯, Gaussian 분포의 Marginal Distribution 역시 Gaussian 분포를 따른다. 2.3.1 에서의 partition을 유지한다면, Marginal Distribution $p(\mathbf{x}_a)$ 는</p>

\[\begin {aligned} E[\mathbf{x}_a] &amp;= \mathbf{u}_a \\\\ Cov[\mathbf{x}_a] &amp;= \Sigma_{aa} \end {aligned}\]

<p>의 평균과 분산을 갖는 Gaussian Distribution을 따른다. 이는 굉장히 직관적이고 편한 결과임을 알 수 있다.</p>

<h3 id="233-bayes-theorem-for-gaussian-variables">2.3.3 Bayes’ theorem for Gaussian variables</h3>

<p>2.3.1 에서 Conditional Distribution 
$p(\mathbf{x}_a|\mathbf{x}_b)$
의 평균 은 
$\mathbf{x}_b$
의 Linear Function으로 나타나고 분산은 
$\mathbf{x}_a$
와 무관하다는 것을 밝혔다. 그렇다면 동일하게 Gaussian r.v. 들이</p>

\[\begin {aligned} p(\mathbf{x}) &amp;= N(\mathbf{x}|\mathbf{u},\Lambda^{-1}) \\\\ p(\mathbf{y|x}) &amp;= N(\mathbf{y}|A\mathbf{x} + \mathbf{b}, \mathbf{L}^{-1}) \end {aligned}\]

<p>로 주어진 상횡 (여기서 $\Lambda$ 와 $\mathbf{L}$ 은 분산의 역행렬인 Precision Matrix를 의미한다.) 에서의 Marginal Distribution $p(\mathbf{y})$ 와 Conditional Distribution 
$p(\mathbf{x|y})$
는 어떻게 나타날까? 이는 조건부 분포의 정의와 Bayes Theorem을 이용하여 구할 수 있는데, 그 결과는</p>

\[\begin {aligned} p(\mathbf{y}) &amp;= N(\mathbf{y}|A\mathbf{u} + \mathbf{b}, \mathbf{L}^{-1} + A\Lambda^{-1}A^T) \\
p(\mathbf{x|y}) &amp;= N(\mathbf{x}|\Sigma\{A^T\mathbf{L}(\mathbf{y}-\mathbf{b})+\Lambda\mathbf{u}\},\Sigma) \end {aligned}\]

<p>where</p>

\[\Sigma = (\Lambda + A^T\mathbf{L}A)^{-1}\]

<p>로 주어진다.</p>

<h3 id="234-maximum-likelihood-for-the-gaussian">2.3.4 Maximum Likelihood for the Gaussian</h3>

<p>Gaussian 분포에서도 MLE를 구할 수 있는데, likelihood function 에 log를 씌운 log-likelihood function를 구한 뒤, log-likelihood function를 최대로 해주는 값을 찾아주는 클래식한 접근을 사용한다.</p>

<p>먼저, log-likelihood는</p>

\[lnp(X|\mathbf{u},\Sigma) = -\cfrac{ND}{2}ln(2\pi) - \cfrac{N}{2}ln|\Sigma| - \cfrac{1}{2}\sum_{i=1}^{N}(\mathbf{x}_i-\mathbf{u})^T\Sigma^{-1}(\mathbf{x}_i-\mathbf{u})\]

<p>로 주어지는데, 여기서 Factorization Theorem을 통해 
<span>$\sum_{i=1}^{N} \mathbf{x}_i$</span>
와 
<span>$\sum_{i=1}^{N} \mathbf{x}_i\mathbf{x}_i^{T}$</span>
가 Sufficient Statistics 라는 것을 알 수 있다. 이후 해당 log-likelihood function를 최대화 해주는 $\mathbf{u}$ 와 $\Sigma$ 의 MLE를 찾아주면,</p>

\[\begin {aligned} \mathbf{u}_{ML} &amp;= \cfrac{1}{N}\sum_{i=1}^{N}\mathbf{x_i} \\\\ \Sigma_{ML} &amp;= \cfrac{1}{N}\sum_{i=1}^N(\mathbf{x}_i-\mathbf{u}_{ML})^T(\mathbf{x}_i-\mathbf{u}_{ML}) \end {aligned}\]

<p>의 값을 가진다는 것을 알 수 있다.</p>

<p>여기서, 정규분포의 MLE에서 잘 알려진 사실인 
<span>$\mathbf{u}_{ML}$</span>
은 unbiased estimator 이지만, 
<span>$\Sigma_{ML}$</span>
은 biased estimator 라는 것을 알 수 있다. 분산에 대한 unbiased estimator를 구하기 위해서는 $N$이 아닌 $(N-1)$로 나눠줘야 한다.</p>

<h3 id="235-sequential-estimation">2.3.5 Sequential estimation</h3>

<p>위에서 구한 평군에 대한 MLE의 식을 조금만 변형하면,</p>

\[\begin {aligned} \mathbf{u}_{ML}^{(N)} &amp;= \cfrac{1}{N}\sum_{i=1}^{N}\mathbf{x_i} \\ &amp;= \mathbf{u}_{ML}^{(N-1)} + \cfrac{1}{N}(\mathbf{x}_N - \mathbf{u}_{ML}^{(N-1)}) \end {aligned}\]

<p>라는 것을 알 수 있다. 즉, 새로운 데이터가 들어오는 대로 MLE의 추정치를 update하는 Sequential Estimation이 가능하다는 것이다. 이러한 Sequential Learning을 일반화 한 것이 <em>Robbins-Monro</em> Algorithm 이다.</p>

<p>Joint r.v. $(z,\theta)$ 가 있다고 하자. 이때, $z$의 조건부 기댓값은 $\theta$에 대한 function으로 나타나며,</p>

\[f(\theta) = E[z|\theta]\]

<p>로 쓸 수 있다. 이렇게 정의된 function을 <em>regression function</em> 이라고 부른다. 이제 목표는 
<span>$f(\theta^{*}) = 0$</span>
을 만족하는, $f$의 근 
<span>$\theta^{*}$</span>
를 찾는 것이다. 이때, z의 조건부 분산이 유한하다고 가정하고, WOLG (without loss of generality) 다음과 같이 근보다 작은 값에서는 음수 값을, 근보다 큰 값에서는 양수 값을 갖는 케이스를 생각하자.</p>

<p><img src="/assets/img/2021-09-21-prml-스터디-chap-2-2/f(theta).jpeg" alt="" title="f(theta)" /></p>

<p>그렇다면, <em>Robbins-Monro</em> Algorithm 은 Positive Sequence ${a_N}$ 을 통해 $\theta^*$ 에 대한 Sequential Estimation을 가능하게 해준다.</p>

\[\theta^{(N)} = \theta^{(N-1)} + a_{N-1}z(\theta^{(N-1)})\]

<p>여기서, $z(\theta^{(N)})$ 는 $\theta$가 $\theta^{(N)}$ 의 값을 가졌을 때의 z 값을 의미한다. 또한, ${a_N}$ 은 다음과 같은 3가지 조건을 만족해야 한다.</p>

<ol>
  <li>$\lim_{N \to \infty} a_N = 0$</li>
  <li>$\sum_{N=1}^{\infty}a_N = \infty$</li>
  <li>$\sum_{N=1}^{\infty}a^2_N &lt; \infty$</li>
</ol>

<p>이 모든 조건이 만족되면, 위의 Sequential Estimation이 실제 근으로 수렴한다는 것이 증명되어있다.</p>

<p>그럼 이제, <em>Robbins-Monro</em> Algorithm을 이용하여 MLE에 대한 Sequential Estimation을 해보자. MLE의 정의에 따라,</p>

\[\cfrac{\partial}{\partial\theta}\biggl\{\cfrac{1}{N}\sum_{i=1}^{N}ln\ p(\mathbf{x}_i|\theta)\biggr\}\biggl|_{\hat{\theta}} = 0\]

<p>임을 알 수 있는데, 여기서 합과 미분의 순서를 바꿔주고 $N$을 극한으로 보내면 대수의 법칙에 따라</p>

\[\lim_{N \to \infty}\cfrac{1}{N}\sum_{i=1}^{N}\cfrac{\partial}{\partial\theta}ln\ p(x_{i}|\theta) = E\biggl[\cfrac{\partial}{\partial\theta}ln\ p(x|\theta) \biggr]\]

<p>임을 알 수 있다. 즉, MLE를 구하는 과정 역시 regression function의 근을 찾는 것으로 생각할 수 있다는 것이다. 이에, <em>Robbins-Monro</em> Algorithm을 적용하면</p>

\[\theta^{(N)} = \theta^{(N-1)} + a_{N-1}\cfrac{\partial}{\partial\theta^{(N-1)}}ln\ p(x_N|\theta^{(N-1)})\]

<p>으로 MLE를 Sequential 하게 구할 수 있음을 알 수 있다.</p>

<h3 id="236-bayesian-inference-for-the-gaussian">2.3.6 Bayesian Inference for the Gaussian</h3>

<p>앞선 포스트에서 Bayesian Inference를 소개하며, Prior, Posterior 등의 개념을 설명한바 있다. Gaussian 분포에도 당연히 Bayesian Inference가 적용될 수 있는데, 첫번째로 분산이 알려진 상황에서 평균에 대한 베이즈 추론을 하는 경우를 생각해보자. (Univariate의 경우)</p>

<p>Conjugate Distribution이 되기 위한 2가지 조건을 생각해보면, 평균에 대한 베이즈 추론에서 사용하는 Prior는</p>

\[p(u) = N(u|u_0,\sigma_0^2)\]

<p>가 적합하다. 여기서 $u_0$ 와 $\sigma_0^2$ 는 임의의 hyperparameter로 볼 수 있다. 이후, Posterior 를 계산해보면,</p>

\[p(u|X) = N(u|u_N,\sigma_N^2)\]

<p>where</p>

\[\begin {aligned} u_N &amp;= \cfrac{\sigma^2}{N\sigma_0^2 + \sigma^2}u_0 + \cfrac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}{u_{ML}} \\\\ \cfrac{1}{\sigma_N^2} &amp;= \cfrac{1}{\sigma_0^2} + \cfrac{N}{\sigma^2} \end {aligned}\]

<p>가 된다.</p>

<p>여기서 앞에서 Binomial 분포와 Multinomial 분포에서 알 수 있었던 베이즈 추론의 일반적인 현상이 또 한번 나타나는데, 바로 N이 커지면 커질수록 Prior 의 영향력이 작아지고 MLE의 영량력이 커지는 방향으로 움직인다는 것이다. 또한, $\sigma_0^2$ 가 무한대로 가면 (Prior에서 $u_0$ 에 대한 확신이 없으면) $u_N$ 은 MLE로 수렴하고, $\sigma_N^2 = \sigma^2 / N$ 으로 나타남을 알 수 있다.</p>

<p>다음은 평균이 알려져있고 분산을 추정해야하는 경우를 생각해보자. 이때, 계산상 편의를 위해 분산의 역수인 Precision $(\lambda \equiv \cfrac{1}{\sigma^2})$ 을 추정하는 것으로 문제를 변경한다. 역시 Conjugate Distribution이 되기 위한 2가지 조건을 생각해보면, Prior로</p>

\[Gam(\lambda|a,b) = \cfrac{1}{\Gamma(a)}b^a\lambda^{a-1}exp(-b\lambda)\]

<p>가 적합하다는 것을 생각할 수 있다. (분산을 직접 추정하고 싶다면 <em>Inverse Gamma</em> 분포를 사용하면 된다.) 해당 Prior를 이용하여 Posterior를 계산해보면,</p>

\[p(\lambda|X) = Gam(\lambda|a_N,b_N)\]

<p>where</p>

\[\begin {aligned} a_N &amp;= a_0 + \cfrac{N}{2} \\\\ b_N &amp;= b_0 + \cfrac{N}{2}\sigma_{ML}^2 \end {aligned}\]

<p>가 된다.</p>

<p>$a_N$ 을 통해 Data Point 하나를 더 관측하는 것은 1/2 만큼의 영향을 준다는 것을 알 수 있는데, 이를 통해 $a_0$ 는 $2a_0$ 개의 ‘effective’ prior observation 으로 해석할 수 있다. 비슷하게, $b_N$ 을 통해 $b_0$ 를 variance $2b_0/2a_0 = b_0/a_0$ 을 가지는 $2a_0$ 개의 ‘effective’ prior observation 으로 인해 발생한 parameter로 해석할 수 있다. 앞서 Dirichlet Prior 에서도 이와 유사한 해석을 한적이 있는데, 사실 Normal과 Multinomial 분포는 모두 Exponential Family에 속하는 분포로 이러한 해석은 Exponential Family 에 속한 분포들에게 흔히 적용할 수 있는 해석이다.</p>

<p>이제, 평균과 분산을 모두 몰라서 동시에 추정해야하는 경우를 생각해보자. 이 경우 Conjugate의 2가지 조건을 만족하는 분포로</p>

\[p(u,\lambda) = N(u|u_0,(\beta\lambda)^{-1})Gam(\lambda|a,b)\]

<p>를 사용하는데, 이 분포는 Normal-gamma 혹은 Gaussian-gamma로 불리는 분포이다. (2개의 독립적인 Normal 분포와 Gamma 분포의 곱이 아니라, $u$의 precision이 $\lambda$ 의 linear function으로 주어진다라는 것에 주의하자)</p>

<p>이제, Multivariate Gaussian의 경우 각 prior를 Multivariate version으로 동일하게 확장해주면 된다. 첫번째 평균만을 위한 Prior는 Normal에서 Multivariate Normal로, 두번째 분산만을 위한 Prior는 Gamma 분포의 Multivariate version 인 Wishart Distribution으로, 마지막으로 평균과 분산 모두를 위한 Prior는 Normal-Wishart 혹은 Gaussian-Wishart 분포를 사용하면 된다.</p>

<h3 id="237-students-t-distribution">2.3.7 Student’s t-distribution</h3>

<p>2.3.6에서 Gaussian의 Precision을 위한 Prior는 Gamma 분포로 주어지는 것을 확인했다. 이때, Univariate Gaussian과 그 Precision의 Prior인 Gamma 분포의 joint p.d.f.에서 Precision을 적분해보자. 그럼 $x$ 의 Marginal Distribution은</p>

\[\begin {aligned} p(x|u,a,b) &amp;= \int_0^{\infty}N(x|u,\tau^{-1})Gam(\tau|a,b)d\tau \\\\ &amp;= \cfrac{b^a}{\Gamma(a)}\biggl({\cfrac{1}{2\pi}}\biggr)^{1/2}\biggl[b + \cfrac{(x-u)^{2}}{2}\biggr]^{-a-1/2}\Gamma(a+1/2) \end {aligned}\]

<p>으로 나타날 것이고, 여기서 $\nu = 2a$, $\lambda = a/b$ 로 두면,</p>

\[St(x|u,\lambda,\nu) = \cfrac{\Gamma(\nu/2 + 1/2)}{\Gamma(\nu/2)}\biggl(\cfrac{\lambda}{\pi\nu}\biggr)^{1/2}\biggl[1 + \cfrac{\lambda(x-u)^2}{\nu}\biggr]^{-\nu/2-1/2}\]

<p>의 p.d.f.를 갖는 Student’s t-distribution이 나타난다. ($\lambda =1,u=0$ 이면 우리가 흔히 아는 t-distribution이 된다.)</p>

<p>여기서 $\nu$ 는 degree of freedom 으로 불리는 parameter 인데, df가 무한대로 가면 
<span>$St(x|u,\lambda,\nu)$</span>
는 
<span>$N(x|u,\lambda^{-1})$</span>
로 분포수렴한다는 것은 유명한 사실이다.</p>

<p>Student’s t-distribution은 그 생성 방법으로부터 동일한 평균 $u$ 를 가졌지만 다른 Precision $\tau$ 을 가지는 정규분포를 무한개 더한 것으로 이해할 수 있는데, (적분을 함수간의 내적으로 해석하면, 평균 $\mu$, Precision $\tau$ 를 가지는 정규분포들을 0부터 $\infty$ 까지 $\tau$의 확률을 weight로 주어 Sum 해준것으로 볼 수 있다. ) Student’s t-distribution은 infinite mixture of Gaussian으로 해석할 수 있다. 그 결과 Student’s t-distribution은 일반적으로 Gaussian 분포보다 longer tail을 갖게 되고, Outlier 몇개에 크게 영향을 받지 않는 Robustness 라는 특성을 가지게 된다. 이로인해 Normal 분포 대신 Student’s t-distribution을 사용하는 Robust Regression도 존재한다.</p>

<p>Multivariate Gaussian이 존재하듯 Student’s t-distribution 도 Multivariate version이 존재하는데, Univariate Gaussian이 아닌 Multivariate Gaussian의 Sum으로 생성할 수 있으며</p>

\[St(\mathbf{x}|\mathbf{u},\Lambda,\nu) = \cfrac{\Gamma(D/2 + \nu/2)}{\Gamma(\nu/2)}\cfrac{|\Lambda|^{1/2}}{(\pi\nu)^{D/2}}\biggl[1 + \cfrac{\bigtriangleup^2}{\nu}\biggr]^{-D/2 -\nu/2}\]

<p>의 p.d.f. 를 가진다. 여기서 $D$ 는 $\mathbf{x}$ 의 Dimension을, $\bigtriangleup^2$ 은 squared Mahalanobis distance를 의미한다.</p>

<p>Multivariate Student’s t-distribution 의 평균과 공분산은 다음과 같다.</p>

\[\begin {aligned}E[\mathbf{x}] &amp;= \mathbf{u} \\\\ cov[\mathbf{x}] &amp;= \cfrac{\nu}{(\nu - 2)}\Lambda^{-1} \end {aligned}\]

<p>(단, 평균은 $\nu &gt; 1$ 인 경우, 분산은 $\nu &gt; 2$ 인 경우 정의된다.)</p>

<h3 id="238-periodic-variables">2.3.8 Periodic variables</h3>

<p>때때로, 주기성을 가지는 변수에 대한 확률 모델을 construct 해야하는 경우가 있다. 예를 들어 방향, 시간 등의 값을 갖는 변수는 그 특성상 주기성을 가질 수 있다. 특히, 공간과 시간을 다루는 Spatio-Temporal Statistics에서는 이러한 변수를 다뤄야 할 일이 많이 발생한다. 이번 절에서는 이러한 주기성을 가지는 확률변수를 위한 분포인 Von Mises Distribution을 Gaussian 분포를 이용하여 어떻게 생성해내는지 다룰 것이다. (이번 절에서는 Univariate 한 경우만 다룬다.)</p>

<p>주기성을 다루자면 자연스럽게 $2\pi$의 주기를 가지는 분포 $p(\theta)$를 생각할 수 있다. 해당 $p(\theta)$는 3가지 조건을 만족해야 한다.</p>

<ol>
  <li>$p(\theta) \geq 0$</li>
  <li>$\int_0^{2\pi}p(\theta) = 1$</li>
  <li>$p(\theta + 2\pi) = p(\theta)$</li>
</ol>

<p>위 3가지 조건을 만족하는 분포를 생성하기 위해 먼저 평균 $\mathbf{u} = (u_1,u_2)$를 가지고, 공분산 행렬 $\Sigma = \sigma^2I$를 가지는 2차원 정규분포를 생각해보자. 그렇다면, 해당 분포는 2차원 공간에서 원 모양의 contour를 가질 것이고, 이 중 고정된 반지름을 가지는 원에 걸치는 부분만 생각한다면 그 부분의 분포는 당연히 주기성을 띄게 될 것이다. 분포의 p.d.f. 를 구하기 위해 먼저 $(x_1,x_2)$ 좌표를 극좌표 $(r,\theta)$로 변환하면 기초 미적분에서 배우듯이 $x_1 = rcos\theta$ , $x_2 = rsin\theta$ 로 변환된다. 이에 따라, 평균 $\mathbf{u}$ 또한 $\mathbf{u_1} = r_0cos\theta_0$, $\mathbf{u_2} = r_0sin\theta_0$ 로 변환될 수 있다.</p>

<p>이제 2차원 정규분포의 p.d.f. 를 극좌표로 변환하고, 위에서 말한대로 고정된 반지름 $r =1$ 인 경우만 생각하면 주기성을 가지는 분포를 만들어낼 수 있다. 해당 과정을 그림으로 보면 다음과 같다.</p>

<p><img src="/assets/img/2021-09-21-prml-스터디-chap-2-2/Figure%201.jpeg" alt="" title="Figure 1" /></p>

<p><img src="/assets/img/2021-09-21-prml-스터디-chap-2-2/Figure%202.jpeg" alt="" title="Figure 2" /></p>

<p>첫번째 그림에서, $\overline{\mathbf{x}} = (\overline{r}cos\overline{\theta},\overline{r}sin\overline{\theta})$ 으로 나타나는 것을 알 수 있고,</p>

\[\begin {aligned}
\overline{r}cos\overline{\theta} = \cfrac{1}{N}\sum_{i=1}^{N}cos\theta_i &amp;&amp;
\overline{r}sin\overline{\theta} = \cfrac{1}{N}\sum_{i=1}^{N}sin\theta_i 
\end {aligned}\]

<p>를 통해서,</p>

\[\overline{\theta} = tan^{-1}\biggl\{\cfrac{\sum_isin\ \theta_i}{\sum_icos\ \theta_i}\biggr\}\]

<p>임을 알 수 있는데, 이는 곧 Von Mises 분포의 $\theta_{0}^{ML}$ 과 같은 값이라는 것이 곧 밝혀질 것이다.</p>

<p>그리고, 두번째 그림에서 Gaussian의 Exponential Part 만 고려하면 (실제 변수는 Exponential Part 에만 존재하므로)</p>

\[\begin {aligned} -\cfrac{(x_1 - u_1)^2 + (x_2-u_2)^2}{2\sigma^2} &amp;= -\cfrac{1}{2\sigma^2}\{(r\cos\theta - r_0\cos\theta_0)^2+(r\sin\theta - r_0\sin\theta_0)^2\} \\\\ &amp;\stackrel{r=1}{=} \cfrac{r_0}{\sigma^2}cos(\theta - \theta_0) + const \end {aligned}\]

<p>로 정리된다. (여기서 초점은 변수 변환 그 자체가 아니라 주기성을 띄는 새로운 p.d.f. 를 만드는 것이기에 자코비안도 무시한다.) 이제 $m = r_0/\sigma^2$로 정의하고 위의 2번째 조건을 만족하기 위해 Normalizing Constant를 넣어주면,</p>

\[p(\theta|\theta_0,m) = \cfrac{1}{2{\pi}I_0(m)}exp\ \{m\cos(\theta - \theta_0)\}\]

<p>로 나타는데, 이 p.d.f. 가 Von Mises 분포의 p.d.f. 이며, $\theta_0$ 는 평균을 나타내는 parameter이며, $m$ 은 Concentration parameter로 Gaussian에서 Precision에 대응된다.</p>

<p>여기서, $I_0(m) = \int_0^{2\pi}exp\ {m\ cos\theta}d\theta$ 로 zeroth-order Bessel function of the first kind 이다.</p>

<p>MLE 추정을 위해 log-likelihood function을 계산하고 $\theta_0$ 와 $m$으로 미분한 값을 0으로 두면, $\theta_0$의 MLE는</p>

\[\theta_0^{ML} = tan^{-1}\biggl\{\cfrac{\sum_{i}sin\theta_{i}}{\sum_{i}cos\theta_{i}}\biggr\}\]

<p>으로 나타나고, (위에서 언급한 $\overline{\theta}$ 와 동일한 값이 된다.) $m$의 MLE는</p>

\[A(m_{ML}) = \cfrac{1}{N}\sum_{i = 1}^{N}cos(\theta_{i} - \theta_0^{ML})\]

<p>where</p>

\[A(m) = \cfrac{I_1(m)}{I_0(m)}\]

<p>로 나타난다. ($I_0’(m) = I_1(m)$ 을 이용)</p>

<p>여기서, 삼각함수 공식 $cosA\ cosB + sinA\ sinB = cos(A-B)$ 를 적용하면,</p>

\[A(m_{ML}) = \biggl(\cfrac{1}{N}\sum_{i =1}^{N}cos\theta_i\biggr)cos\theta_0^{ML} + \biggl(\cfrac{1}{N}\sum_{i =1}^{N}sin\theta_i\biggr)sin\theta_0^{ML}\]

<p>로 나타낼 수 있고 $\overline{r}$, $\overline{\theta}$ 를 이용하여</p>

\[A(m_{ML}) = \overline{r}\]

<p>로 정리됨을 알 수 있다. 이제 $A^{-1}$ 를 양측에 취해주면 $m_{ML}$ 을 구할 수 있다.</p>

<h3 id="239-mixture-of-gaussians">2.3.9 Mixture of Gaussians</h3>

<p>이전에, Gaussian 분포는 Unimodal 하기 때문에 Multimodal 한 데이터를 표현하기 위해 Mixture of Gaussians를 배워야 한다고 했었다. Mixture Model은 여러 분포의 Linear Combination을 의미하는데, Mixture of Gaussians는 그 중에서 다음과 같이 여러 Gaussian 분포를 선형결합한 형태이다.</p>

\[p(\mathbf{x}) =  \sum_{k=1}^{K}\pi_kN(\mathbf{x}|\mathbf{u}_k,\Sigma_k)\]

<p>여기서, 각 
<span>$N(\mathbf{x}|\mathbf{u}_k,\Sigma_k)$</span>
은 <em>component</em>, 
<span>$\pi_k$</span>
는 <em>mixing coefficient</em> 로 불린다. 이렇게 형성된 Mixture of Gaussians를 시각화한 예는 다음과 같다.</p>

<p>Mixing Coefficient $\pi_k$ 는 Normalization을 위해서 다음 2개 조건을 만족해야 한다.</p>

<ol>
  <li>$\sum_{k = 1}^{K}\pi_k = 1$</li>
  <li>$0 \leq \pi_k \leq1$</li>
</ol>

<p>이는 확률값이 가져야 하는 조건과 흡사한데, 실제로</p>

\[p(\mathbf{x}) = \sum_{k=1}^{K}p(k)p(\mathbf{x}|k)\]

<p>라는 사실에서 알 수 있듯 
<span>$\pi_k$</span>는
k 번째 component를 뽑을 prior probability $p(k)$, 
<span>$N(\mathbf{x}|\mathbf{u}_k,\Sigma_k)$</span>
는 k에 조건부가 걸린 $\mathbf{x}$의 확률 
$p(\mathbf{x}|k)$
로 이해할 수 있다. 이에, Mixture of Gaussian을 생성하는 Sampling Simulation은 각 component에서 $\pi_k$에 의해 정해진 확률대로 sample을 뽑는 방식으로 진행된다. 또한, 반대로 데이터가 주어졌을 때 해당 데이터가 어느 component에서 나왔는지에 대한 확률인 <em>responsibilities</em> (
<span>$\gamma_k(\mathbf{x}) \equiv p(k|\mathbf{x})$</span>
) 역시 중요한 의미를 가지는데, 이는 이후 클러스터링 기법 중 하나인 GMM (Gaussian Mixture Model)과 큰 관계가 있다.</p>

<p>Mixture of Gaussian의 MLE는 단일 Gaussian에 비해 구하기가 매우 힘든데, 보통은 나중에 다룰 EM Algorithm을 통해 구한다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#machine-learning" class="page__taxonomy-item p-category" rel="tag">Machine Learning</a><span class="sep">, </span>
    
      <a href="/tags/#prml" class="page__taxonomy-item p-category" rel="tag">PRML</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#prml-%EC%8A%A4%ED%84%B0%EB%94%94" class="page__taxonomy-item p-category" rel="tag">PRML 스터디</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2021-09-21T00:00:00+09:00">September 21, 2021</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=PRML+%EC%8A%A4%ED%84%B0%EB%94%94+Chap.2+%282%29%20http%3A%2F%2Flocalhost%3A4000%2Fprml%2520%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594%2Fprml-%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594-chap-2-2%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fprml%2520%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594%2Fprml-%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594-chap-2-2%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fprml%2520%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594%2Fprml-%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594-chap-2-2%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-2-1/" class="pagination--pager" title="PRML 스터디 Chap.2 (1)
">이전</a>
    
    
      <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-3-3/" class="pagination--pager" title="PRML 스터디 Chap.3.3
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-6-4/" rel="permalink">PRML 스터디 Chap.6.4
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">6.4.5 Gaussian processes for classification
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-6-1-2/" rel="permalink">PRML 스터디 Chap.6.1-2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">6. Kernel Methods
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-7/" rel="permalink">PRML 스터디 Chap.5.7
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">5.5.5 Training with transformed data
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-4/" rel="permalink">PRML 스터디 Chap.5.4
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">5.4 The Hessian Matrix
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 cwyoon96. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>









<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'] ],
		processEscapes: true
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
});
</script>


  </body>
</html>
