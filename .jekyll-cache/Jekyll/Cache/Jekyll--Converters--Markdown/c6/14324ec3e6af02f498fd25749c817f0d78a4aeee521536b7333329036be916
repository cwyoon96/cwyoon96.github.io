I"! <p>(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.)</p>

<p>PRML 3.3절의 주제는 Bayesian Linear Regression이다. 이전 3.1, 3.2절에서 다뤘던 Linear Regression을 베이지안의 관점에서 모델링한 결과를 보여준다. Bayesian Linear Regression은 특히 더 뒤에 Chapter에서 다룰 Gaussian Process와 깊은 관련이 있으니 열심히 공부하도록 하자.</p>

<h2 id="33-bayesian-linear-model">3.3 Bayesian Linear Model</h2>

<p>기존의 Linear Regression 은 Maximum Likelihood Estimation에 의존하기 때문에, Model Complexity를 제어하기 힘들며, overfitting이 쉽게 일어날 수 있다. 이를 해결하기 위한 Regularization 방법도 이전에 다루었지만, 3.3절에서는 이를 Bayesian Perspective를 통해 해결한다.</p>

<h3 id="331-parameter-distribution">3.3.1 Parameter distribution</h3>

<p>Bayesian Linear Model의 가장 큰 특징은 model parameter인 $\mathbf{w}$를 random variable로 보고 분포를 가정한다는 것이다. 먼저, precision parameter $\beta$가 알려져있다고 하자. Likelihood Function 
$p(t|\mathbf{w}) = N(t|y(\mathbf{x},\mathbf{w}),\beta^{-1})$
이 $\mathbf{w}$에 대해 제곱의 지수꼴을 하고 있기 때문에, $\mathbf{w}$의 conjugate prior는 정규분포가 적합할 것이다.</p>

\[p(\mathbf{w}) = N(\mathbf{w}|\mathbf{m}_0,\mathbf{S}_0)\]

<p>이를 바탕으로 posterior 분포를 계산해보면 (Chap.2 에서의 공식을 이용하여)</p>

\[p(\mathbf{w}|\mathbf{t}) = N(\mathbf{w}|\mathbf{m}_N,\mathbf{S}_N)\]

<p>where</p>

\[\begin {aligned}
\mathbf{m}_N &amp;= \mathbf{S}_N(\mathbf{S}_{0}^{-1}\mathbf{m}_0 + \beta\mathbf{\Phi}^{T}\mathbf{t}) \\ \mathbf{S}_{N}^{-1} &amp;= \mathbf{S}_{0}^{-1} + \beta\mathbf{\Phi}^{T}\mathbf{\Phi}
\end {aligned}\]

<p>임을 알 수 있다.</p>

<p>해당 식을 통해 
<span>$\mathbf{w}_{MAP} = \mathbf{m}_N$</span>
가 되며, 만약 
<span>$\mathbf{S}_0 = \alpha^{-1}\mathbf{I}$</span> 로 두고 
<span>$\alpha \rightarrow 0$</span>
이면 
<span>$\mathbf{m}_N$</span>
은 
<span>$\mathbf{w}_{ML}$</span>
과 같은 값을 가지게 된다. 또한, 
<span>$N = 0$</span>
이면 posterior 분포는 prior와 같은 분포가 된다. 이는 앞서서 보았던 posterior 분포의 일반적 특성과 일치한다.</p>

<p>이제 prior 분포가</p>

\[p(\mathbf{w}|\alpha) = N(\mathbf{w}|0,\alpha^{-1}\mathbf{I})\]

<p>인 단순한 케이스를 생각해보자. 그러면 posterior 분포는</p>

\[\begin {aligned}
\mathbf{m}_N &amp;= \beta\mathbf{S}_N\mathbf{\Phi}^{T}\mathbf{t} \\ \mathbf{S}_{N}^{-1} &amp;= \alpha\mathbf{I} + \beta\mathbf{\Phi}^{T}\mathbf{\Phi}
\end {aligned}\]

<p>의 평균과 분산을 가진다. 그리고 log posterior를 $\mathbf{w}$에 대해서 정리하면,</p>

\[ln\,p(\mathbf{w}|\mathbf{t}) = -\cfrac{\beta}{2}\sum_{n = 1}^{N}\{t_n-\mathbf{w}^{T}\phi(\mathbf{x}_n)\}^{2}-\cfrac{\alpha}{2}\mathbf{w}^{T}\mathbf{w} + const\]

<p>으로 주어진다. 그러므로 posterior를 최대화하는 것은 sum-of-squares error를 최소화하면서 동시에 regularization term이 존재하는 것과 동일하다는 것을 알 수 있다.</p>

<p>Bayesian Approach의 큰 장점 중 하나는 이전에도 다루었듯이 Sequential update가 가능하다는 것이다. 아래의 그림은 $y(x,\mathbf{w}) = w_0 + w_1x$ 이라는 간단한 식을 두고 $w_0,w_1$ 을 반복적으로 update 하는 모습을 보여준다.</p>

<p><img src="/assets/img/2021-11-14-prml-스터디-chap-3-3/Figure%203.7.png" alt="" /></p>

<p>해당 그림을 통해, posterior 분포가 원래의 실제 값인 하얀 십자가에 점점 가까워지는 것을 관측할 수 있고, 선형 식도 데이터에 더 적합하도록 변화하는 것을 볼 수 있다.</p>

<h3 id="332-predictive-distribution">3.3.2 Predictive distribution</h3>

<p>실제 linear regression 문제에서 우리가 관심을 가지는 부분은 $\mathbf{w}$보다는 새로운 데이터가 들어왔을 때 $t$를 정확히 예측하는 것이다. 이를 위해 predictive distribution을 계산하면,</p>

\[p(t|\mathbf{t},\alpha,\beta) = \int{p(t|\mathbf{w},\beta)p(\mathbf{w}|\mathbf{t},\alpha,\beta)d\mathbf{w}}\]

<p>가 되어 ($t$는 예측하고자 하는 target, $\mathbf{t}$는 fitting을 위한 데이터로 주어진 값)</p>

\[p(t|\mathbf{x},\mathbf{t},\alpha,\beta) = N(t|\mathbf{m}_N^T\phi(\mathbf{x}),\cfrac{1}{\beta} +\phi(\mathbf{x})^{T}\mathbf{S}_N\phi(\mathbf{x}))\]

<p>로 주어진다. ($\mathbf{x}$는 예측을 위해 새로 주어진 value) 여기서, 뒤의 분산항을 살펴보면 데이터가 더 많이 관측될 수록 분산이 줄어듦을 알 수 있으며, $N$이 무한대로 가면 순수 noise인 $\beta^{-1}$ 만 남는다는 것을 알 수 있다. 아래의 그림은 Sequential update를 통해 predictive distribution이 어떻게 변화하는지 보여준다. (초록 선이 true model)</p>

<p><img src="/assets/img/2021-11-14-prml-스터디-chap-3-3/Figure%203.8.png" alt="" /></p>

<p>이때, Gaussian과 같은 localized basis function을 사용하면 basis function의 중심에서 벗어난 region에서는 분산항의 2번째 term이 0으로 수렴해서 $\beta^{-1}$ 만 남는 상황이 발생한다. 즉, extraploation에 대해서 굉장히 낮은 variance를 가지게 되는 것인데 이러한 문제는 향후에 소개될 Gaussian Process를 통해 해결할 수 있다.</p>

<p>또한, 지금까지는 $\beta$가 알려져있다고 가정했으나, $\mathbf{w},\beta$가 모두 알려져있지 않다고 가정하면 Chapter 2에서 소개한 Gaussian-Gamma 분포를 prior 분포로 사용할 수 잇다. 이 경우, predictive 분포는 Student’s t 분포로 나타난다.</p>

<h3 id="333-equivalent-kernel">3.3.3 Equivalent kernel</h3>

<p>위에서 주어진 predictive mean을 다시 쓰면</p>

\[y(\mathbf{x},\mathbf{m}_N) = \mathbf{m}_{N}^{T}\phi(\mathbf{x}) = \beta\phi(\mathbf{x})^{T}\mathbf{S}_{N}\Phi^{T}\mathbf{t} = \sum_{n=1}^{N}\beta\phi(\mathbf{x})^{T}\mathbf{S}_{N}\phi(\mathbf{x}_n)t_{n}\]

<p>으로 주어진다. 즉, predictive mean이 training set의 target values인 $t_n$의 linear combination으로 주어짐을 알 수 있고,</p>

\[y(\mathbf{x},\mathbf{m}_N) = \sum_{n=1}^{N}k(\mathbf{x},\mathbf{x}_n)t_n\]

<p>where</p>

\[k(\mathbf{x},\mathbf{x}') = \beta\phi(\mathbf{x})^{T}\mathbf{S}_N\phi(\mathbf{x}')\]

<p>으로 쓸 수 있다. 여기서 $k(\mathbf{x},\mathbf{x}’)$ 함수는 <em>smoother matrix</em> 혹은 <em>equivalent kernel</em> 이라 불린다. 또한, 이렇게 training set의 target values의 linear combination으로 prediction을 하는 회귀식을 <em>linear smoothers</em> 라 부른다.</p>

<p>이때, Gaussian basis function의 equivalent kernel을 살펴보면</p>

<p><img src="/assets/img/2021-11-14-prml-스터디-chap-3-3/3.10.png" alt="" /></p>

<p>의 형태를 가진다는 것을 알 수 있다. 즉, $k(x,x’)$ 에서 서로 비슷한 값을 가질 때 더 큰 값을 가진다는 것인데, 이를 위의 linear lombination과 같이 생각하면 $\mathbf{t}$를 예측하고자 하는 $\mathbf{x}$와 가까운 $\mathbf{x}_n$의 $\mathbf{t}_n$에 더 많은 weight를 부여하는 합리적인 형태라는 것을 알 수 있다. equivalent kernel의 이러한 형태는 다른 Basis function을 사용해도 유지된다. 또한, predictive mean간의 공분산을 계산해보면</p>

\[\begin {aligned}
cov[y(\mathbf{x}),y(\mathbf{x'})] &amp;= cov[\phi(\mathbf{x})^{T}\mathbf{w},\mathbf{w}^{T}\phi(\mathbf{x'})] \\ &amp;= \phi(\mathbf{x})^{T}\mathbf{S}_{N}\phi(\mathbf{x}') = \beta^{-1}k(\mathbf{x},\mathbf{x}')
\end {aligned}\]

<p>으로 주어져, 가까운 지점간의 predictive mean은 상대적으로 큰 Correlation을 가지게 된다.</p>

<p>이렇듯, equivalent kernel을 이용하여 Bayesian Linear Regression의 결과를 나타낼 수 있다는 것에 착안해 Basis Function을 먼저 지정하지 않고, kernel function 만을 가지고 주어진 데이터를 학습하고 새로운 데이터에 대한 예측을 하는 것을 생각해볼 수 있는데, 이는 향후 다룰 Gaussian Process의 Framework가 된다.</p>
:ET