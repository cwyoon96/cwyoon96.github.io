I"ç'<h2 id="54-the-hessian-matrix">5.4 The Hessian Matrix</h2>

<p>ì§€ê¸ˆê¹Œì§€ëŠ” 1ì°¨ ë¯¸ë¶„ê°’ (gradient)ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ backpropagationì„ ì“°ëŠ” ë°©ë²•ì„ ê³µë¶€í–ˆì§€ë§Œ, backpropagationì„ ì´ìš©í•˜ì—¬ 2ì°¨ ë¯¸ë¶„ê°’</p>

\[\cfrac{\partial^2 E}{\partial w_{ji}\partial w_{lk}}\]

<p>ì„ êµ¬í•  ìˆ˜ ë„ ìˆë‹¤. ì´ ë•Œ, 2ì°¨ ë¯¸ë¶„ ê°’ì„ ëª¨ì•„ë‘” matrixë¥¼ Hessian Matrixë¼ê³  ë¶€ë¥´ëŠ”ë°, Hessianì€ ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ë¡œ ì‚¬ìš©ëœë‹¤.</p>

<ol>
  <li>ë‹¤ì–‘í•œ ë¹„ì„ í˜• ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì€ 2ì°¨ ë¯¸ë¶„ê°’ì˜ íŠ¹ì„±ì— ê¸°ë°˜ì„ ë‘ê³  ìˆë‹¤.</li>
  <li>Train dataì— ë³€í™”ê°€ ìƒê²¼ì„ ë•Œ ë¹ ë¥´ê²Œ re-train í•˜ëŠ”ë° 2ì°¨ ë¯¸ë¶„ê°’ì´ ì‚¬ìš©ëœë‹¤.</li>
  <li>Hessianì˜ inverseë¥¼ í†µí•´ weightì˜ ì¤‘ìš”ë„ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.</li>
  <li>Bayesian nueral networkì—ì„œ Laplace approximationì„ í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.</li>
</ol>

<p>Hessianì€ approximationì„ í†µí•´ì„œ êµ¬í•˜ê¸°ë„ í•˜ê³ , backpropagationì„ í†µí•´ exact valueë¥¼ êµ¬í•  ìˆ˜ë„ ìˆë‹¤. ë‹¤ë§Œ, ì „ì²´ $W$ê°œì˜ parameterê°€ ì¡´ì¬í•œë‹¤ê³  í•  ë•Œ, Hessianì˜ ì°¨ì›ì€ $W \times W$ ì´ê¸° ë•Œë¬¸ì— efficient evaluation scaleì€ $O(W^2)$ì´ë‹¤.</p>

<h3 id="541-diagonal-approximation">5.4.1 Diagonal approximation</h3>

<p>ë§ì€ ê²½ìš° Hessian ê·¸ ìì²´ë³´ë‹¤ Hessianì˜ inverseê°€ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ Hessianì˜ diagonalì„ approximateí•´ì„œ off-diagonalì„ 0ìœ¼ë¡œ ë‘ê³  inverseë¥¼ ì‰½ê²Œ êµ¬í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ë‹¹ì—°íˆ ëŒ€ë¶€ë¶„ì˜ Hessianì€ diagonal matrixê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì‚¬ìš©ì‹œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•œë‹¤. Hessianì˜ diagonal elementëŠ”</p>

\[\cfrac{\partial^2 E_n}{\partial w_{ji}^2} = \cfrac{\partial E_n}{\partial a_j^2}z_i^2\]

<p>ë¡œ ì£¼ì–´ì§€ëŠ”ë°, ì•ì„œ ë´¤ë˜ chain ruleì„ ì´ìš©í•˜ë©´</p>

\[\cfrac{\partial E_n}{\partial a_j^2} = h'(a_j)^2 \sum_{k} \sum_{k'}w_{kj}w_{k'j}\cfrac{\partial^2 E_n}{\partial a_k \partial a_{k'}} + h''(a_j) \sum_{k} w_{kj} \cfrac{\partial E_n}{\partial a_k}\]

<p>ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ì œ 2ì°¨ ë¯¸ë¶„ termì—ì„œ off-diagonal termì„ ì§€ìš°ë©´</p>

\[\cfrac{\partial E_n}{\partial a_j^2} = h'(a_j)^2 \sum_{k}w_{kj}^2\cfrac{\partial^2 E_n}{\partial a_k^2} + h''(a_j) \sum_{k} w_{kj} \cfrac{\partial E_n}{\partial a_k}\]

<p>ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ diagonal approximationì˜ costëŠ” $O(W)$ë¡œ, full Hessianì˜ $O(W^2)$ ë³´ë‹¤ í›¨ì”¬ ì ì€ costê°€ ë“ ë‹¤. ë¬¼ë¡ , odff-diagonal termì„ ì§€ìš°ì§€ ì•Šê³  approximateí•  ìˆ˜ ìˆì§€ë§Œ ê·¸ëŸ¬ë©´ $O(W)$ scaleì´ ì•„ë‹ˆê²Œ ëœë‹¤.</p>

<h3 id="542-outer-product-approximation">5.4.2 Outer product approximation</h3>

<p>Single output regression ë¬¸ì œë¥¼ ìƒê°í•˜ë©´ Hessian  $\mathbf{H}$ëŠ”</p>

\[\mathbf{H} = \bigtriangledown \bigtriangledown E = \sum_{n=1}^{N} \bigtriangledown y_n \bigtriangledown y_n + \sum_{n=1}^{N} (y_n - t_n) \bigtriangledown \bigtriangledown y_n\]

<p>ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤. ì´ë•Œ, optimal functionì€  target dataì˜ conditional averageë¥¼ outputìœ¼ë¡œ ì£¼ê¸° ë•Œë¬¸ì— ë’¤ì— 
<span>$y_n - t_n$</span>
ì€ zero meanì„ ê°€ì§€ëŠ” random variableì´ ëœë‹¤. 2ì°¨ ë¯¸ë¶„ê°’ê³¼ uncorrelated ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ë©´ second term ì „ì²´ëŠ” sumì„ ê±°ì¹˜ë©´ì„œ 0ìœ¼ë¡œ approximate ë  ê²ƒì´ë‹¤.</p>

<p>ì´ë ‡ê²Œ</p>

\[\mathbf{H} \backsimeq \sum_{n=1}^N \mathbf{b}_n\mathbf{b}_n^T\]

<p>where <span>$\mathbf{b}_n = \bigtriangledown y_n = \bigtriangledown a_n$</span></p>

<p>í˜•íƒœì˜ approximationì„ outer product approximationì´ë¼ ë¶€ë¥¸ë‹¤. 1ì°¨ ë¯¸ë¶„ê°’ ë§Œì„ ì´ìš©í•˜ì—¬ approximationì„ í•˜ê¸° ë•Œë¬¸ì— ì „ì²´ $O(W^2)$ì˜ costê°€ ë“ ë‹¤. ë¬¼ë¡ , ì‹¤ì œë¡œëŠ” second termì„ ë¬´ì‹œí•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì— ê°€ì •ì²˜ëŸ¼ ì˜ train ëœ networkì— ëŒ€í•´ì„œë§Œ ìœ ì˜ë¯¸í•œ approximationì´ë‹¤.</p>

<p>Cross-entropy error functionì— logistic sigmoid output-uinit activation functionì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°</p>

\[\mathbf{H} \backsimeq \sum_{n=1}^N y_n(1-y_n) \mathbf{b}_n\mathbf{b}_n^T\]

<p>ìœ¼ë¡œ approximate í•  ìˆ˜ ìˆìœ¼ë©° softmaxì˜ ê²½ìš°ì—ë„ í˜•íƒœì— ë§ì¶° approximate í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="543-inverse-hessian">5.4.3 Inverse Hessian</h3>

<p>Outer approximationì„ í†µí•´ Hessianì˜ inverseë¥¼ ì‰½ê²Œ approximateí•  ìˆ˜ ìˆë‹¤. Outer approximationì„</p>

\[\mathbf{H}_N = \sum_{n=1}^{N} \mathbf{b}_n\mathbf{b}_n^T\]

<p>ë¡œ notateí•˜ì. í˜„ì¬ê¹Œì§€ $L$ data pointsë¥¼ í†µí•´ Hessian matrixë¥¼ approximate í•˜ê³  ê·¸ inverseë¥¼ êµ¬í–ˆë‹¤ê³  í•˜ì. ì´ë•Œ $L+1$th data pointë¥¼ ì¶”ê°€í•˜ë©´</p>

\[\mathbf{H}_{L+1} = \mathbf{H}_{L} + \mathbf{b}_{L+1}\mathbf{b}_{L+1}^T\]

<p>ë¡œ ì“¸ ìˆ˜ ìˆë‹¤. ì´ë•Œ, Woodbury identityë¥¼ ì´ìš©í•˜ë©´</p>

\[\mathbf{H}_{L+1}^{-1} = \mathbf{H}_{L}^{-1} - \cfrac{\mathbf{H}_{L}^{-1}\mathbf{b}_{L+1} \mathbf{b}_{L+1}^T \mathbf{H}_{L}^{-1}}{1+ \mathbf{b}_{L+1}^T \mathbf{H}_{L}^{-1} \mathbf{b}_{L+1}}\]

<p>ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ $L+1$ì´ Nì´ ë  ë•Œê¹Œì§€ sequentialí•˜ê²Œ êµ¬í•˜ë©´ Hessian matrixì˜ inverseë¥¼ ì‰½ê²Œ approximateí•  ìˆ˜ ìˆë‹¤. ê°€ì¥ ì²« $\mathbf{H}_0$ì€ ë³´í†µ $\alpha \mathbf{I}$ë¡œ ë‘ê¸° ë•Œë¬¸ì— ì‹¤ì§ˆì ìœ¼ë¡œëŠ” 
$\mathbf{H} + \alpha \mathbf{I}$ì˜ inverseë¥¼ êµ¬í•˜ëŠ” í˜•íƒœê°€ ëœë‹¤.</p>

<h3 id="544-finite-differences">5.4.4 Finite differences</h3>

<p>Hessian ì—­ì‹œ finite differencesë¥¼ í†µí•´ êµ¬í•  ìˆ˜ ìˆëŠ”ë°,</p>

\[\cfrac{\partial^2 E}{\partial w_{ji} \partial w_{lk}} = \cfrac{1}{4 \epsilon^2}\{E(w_{ji} + \epsilon, \ w_{lk} + \epsilon ) - E(w_{ji} + \epsilon, \ w_{lk} - \epsilon ) \\ - E(w_{ji} - \epsilon, \ w_{lk} + \epsilon) + E(w_{ji} - \epsilon, \ w_{lk} - \epsilon ) \} + O(\epsilon^2)\]

<p>ì˜ í˜•íƒœê°€ ëœë‹¤. Hessianì—ëŠ” $W^2$ì˜ elementê°€ ìˆê³ , í•˜ë‚˜ì˜ evaluationì€ $W$ ë§Œí¼ì˜ costê°€ ë“¤ê¸° ë•Œë¬¸ì— ì´ $O(W^3)$ì˜ ocstê°€ ë“¤ì§€ë§Œ ì—¬ì „íˆ backpropagation implementationì˜ checkë¥¼ ìœ„í•´ ì‚¬ìš©ëœë‹¤.</p>

<p>1ì°¨ ë¯¸ë¶„ê°’ì— central differenceë¥¼ ì´ìš©í•´ numerical differentiationì„ ë” íš¨ìš¸ì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ”ë°,</p>

\[\cfrac{\partial^2 E}{\partial w_{ji} \partial w_{lk}} = \cfrac{1}{2\epsilon} \bigg\{ \cfrac{\partial E}{\partial w_{ji}}(w_{lk} + \epsilon) - \cfrac{\partial E}{\partial w_{ji}}(w_{lk} - \epsilon)  \bigg\} + O(\epsilon^2)\]

<p>ì˜ í˜•íƒœê°€ ë˜ë©° ì´ë•ŒëŠ” $O(W^2)$ ë§Œí¼ì˜ costê°€ ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="545-exact-evaluation-of-the-hessian">5.4.5 Exact evaluation of the Hessian</h3>

<p>í˜„ì¬ê¹Œì§€ëŠ” Hessian í˜¹ì€ ê·¸ inverseë¥¼ approximateí•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ì§€ë§Œ, ì‹¤ì œë¡œ Hessianì€ backpropagationì„ í†µí•´ excat evaluationì´ ê°€ëŠ¥í•˜ë©° ê·¸ costëŠ” $O(W^2)$ scaleì´ë‹¤.</p>

<p>ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ input ($i,iâ€™$ index), í•˜ë‚˜ì˜ hidden layer ($j, jâ€™$ index), ê·¸ë¦¬ê³  output ($k,kâ€™$ index)ë¡œ ì´ë£¨ì–´ì§„ networkë¥¼ ìƒê°í•˜ê² ë‹¤. ë¨¼ì €</p>

\[\delta_k = \cfrac{\partial E_n}{\partial a_k}, \; M_{kk'} \equiv \cfrac{\partial^2 E_n}{\partial a_k \partial a_{k'}}\]

<p>ë¥¼ ì •ì˜í•˜ì.  ì´ë•Œ Hessian matrixëŠ” 3ê°œì˜ blockìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ”ë°,</p>

<ol>
  <li>Second layerì—ì„œì˜ weightsì— ëŒ€í•œ 2ì°¨ ë¯¸ë¶„</li>
</ol>

\[\cfrac{\partial^2 E_n}{\partial w_{kj}^{(2)} \partial w_{k'j'}^{(2)}} = z_j z_{j'}M_{kk'}\]

<ol>
  <li>First layerì—ì„œì˜ weightsì— ëŒ€í•œ 2ì°¨ ë¯¸ë¶„</li>
</ol>

\[\cfrac{\partial^2 E_n}{\partial w_{ji}^{(1)} \partial w_{j'i'}^{(1)}} = x_i x_{i'} h''(a_{j'})I_{jj'}\sum_{k} w_{kj'}^{(2)}\delta_{k} \\ + x_i x_{i'}h'(a_{j'})h'(a_j)\sum_{k} \sum_{k'} w_{k'j'}^{(2)} w_{kj}^{(2)}M_{kk'}\]

<ol>
  <li>ê° layerì—ì„œ weight í•œë²ˆì”©</li>
</ol>

\[\cfrac{\partial^2 E_n}{\partial w_{ki}^{(2)}\partial w_{kj'}^{(2)}} = x_i h'(a_{j'}) \bigg\{ \delta_k I_{jj'} + z_j \sum_{k'} w_{k'j'}^{(2)} H_{kk'} \bigg\}\]

<p>ë¡œ ê°ê° êµ¬í•´ì§„ë‹¤.</p>

<h3 id="546-fast-multiplication-by-the-hessian">5.4.6 Fast multiplication by the Hessian</h3>

<p>Hessianì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ caseëŠ” $\mathbf{v}^T\mathbf{H}$ í˜•íƒœì˜ vectorë¥¼ productí•œ ê²½ìš°ì´ë‹¤. ì´ë•Œ $\mathbf{v}^T\mathbf{H}$ëŠ” $O(W)$ scaleì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë°”ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´</p>

\[\mathbf{v}^T\mathbf{H} = \mathbf{v}^T \bigtriangledown (\bigtriangledown E)\]

<p>ì—ì„œ</p>

\[\mathcal{R}\{\cdot \} \equiv \mathbf{v}^T \bigtriangledown (\cdot)\]

<p>notationì„ ì •ì˜í•˜ì. ì˜ˆë¥¼ ë“¤ì–´</p>

\[\mathcal{R}\{\ \mathbf{w} \} = \mathbf{v}\]

<p>ê°€ ë  ê²ƒì´ë‹¤.</p>

<p>ë‹¤ì‹œ two-layer networkì— linear output units, sum-of-square error functionì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë¥¼ ì˜ˆë¡œ ë“¤ì–´ë³´ì. Forward propagationì˜ ê²½ìš°</p>

\[\begin{aligned}
a_j &amp;= \sum_i w_{ji}x_i \\
z_j &amp;= h(a_j) \\
y_k &amp;= \sum_j w_{kj} z_j
\end{aligned}\]

<p>ìœ¼ë¡œ ì •ì˜í–ˆì—ˆëŠ”ë°, ì´ì œ $\mathcal{R}{\cdot}$ operatorë¥¼ ì‚¬ìš©í•˜ì—¬</p>

\[\begin{aligned}
\mathcal{R}\{a_j\} &amp;= \sum_i v_{ji}x_i \\
\mathcal{R}\{z_j\} &amp;= h'(a_j)\mathcal{R}\{a_j\} \\
\mathcal{R}\{y_k\} &amp;= \sum_j w_{kj} \mathcal{R}\{z_j\} + \sum_j v_{kj}z_j
\end{aligned}\]

<p>ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. (ì—¬ê¸°ì„œ $v_{ji}$ëŠ” weight $w_{ji}$ì— ëŒ€ì‘í•˜ëŠ” element)</p>

<p>ì´ì œ bacpropagationìœ¼ë¡œ ëŒì•„ê°€ë©´</p>

\[\begin{aligned}
\delta_k &amp;= y_k - t_k \\
\delta_j &amp;= h'(a_j) \sum_k w_{kj}\delta_k
\end{aligned}\]

<p>ì„ì„ ì•Œê³  ìˆì—ˆë‹¤. ì—¬ê¸°ì„œë„ $\mathcal{R}{\cdot}$ operatorë¥¼ ì‚¬ìš©í•˜ì—¬</p>

\[\begin{aligned}
\mathcal{R}\{\delta_k\} &amp;= \mathcal{R}\{y_k\} \\
\mathcal{R}\{\delta_j\} &amp;= h''(a_j)\mathcal{R}\{a_j\}\sum_{k} w_{kj}\delta_k \\
&amp;+ h'(a_j)\sum_{k} v_{kj}\delta_k + h'(a_j)\sum_{k} w_{kj}\mathcal{R}\{\delta_k\}
\end{aligned}\]

<p>ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ, 1ì°¨ ë¯¸ë¶„ê°’</p>

\[\begin{aligned}

\cfrac{\partial E}{\partial w_{kj}} &amp;= \delta_k z_j \\

\cfrac{\partial E}{\partial w_{ji}} &amp;= \delta_j x_i

\end{aligned}\]

<p>ì—  $\mathcal{R}{\cdot}$ operatorë¥¼ ì‚¬ìš©í•˜ì—¬ vector $\mathbf{v}^T\mathbf{H}$ì˜ ì›ì†Œì— ëŒ€í•œ expressionì„ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>

\[\begin{aligned}

\mathcal{R}\bigg\{ \cfrac{\partial E}{\partial w_{kj}} \bigg\} &amp;=  \mathcal{R}\{\delta_k\} z_j + \delta_k \mathcal{R}\{z_j\} \\


\mathcal{R}\bigg\{ \cfrac{\partial E}{\partial w_{ji}}\bigg\} &amp;= x_{i}\mathcal{R}\{\delta_j\}

\end{aligned}\]

<p>ì´ë•Œ $\mathbf{v}$ì— unit vectorë¥¼ ë„£ì–´ Hessianì˜ elementë„ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>
:ET