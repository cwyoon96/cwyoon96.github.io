I"¢.<h3 id="416-fishers-discriminant-for-multiple-classes">4.1.6 Fisherâ€™s discriminant for multiple classes</h3>

<p>ì´ì œëŠ” ê¸°ì¡´ì˜ Binary Classificationì„ ë„˜ì–´, Fisherâ€™s LDAë¥¼ ì´ìš©í•´ multiple class discriminationì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•˜ëŠ”ì§€ ë‹¤ë£° ê²ƒì´ë‹¤. K(&gt;2) ê°œì˜ classê°€ ìˆê³ , input spaceì˜ dimensionì´ Dë¼ê³  ê°€ì •í•˜ì (D &gt; K). ì´ì „ì˜ Binary Classificationì—ì„œëŠ” 1ì°¨ì›ìœ¼ë¡œ ë°ì´í„°ë¥¼ Project í–ˆì§€ë§Œ, ì´ë²ˆì—ëŠ” ë” í™•ì¥í•˜ì—¬ ì„ì˜ì˜ Dâ€™ ì°¨ì›ìœ¼ë¡œ Projectí•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£° ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ weight vector $\mathbf{w}_d$ (d = 1,â€¦,Dâ€™)ë¥¼ ì •ì˜í•˜ì—¬ Dâ€™ê°œì˜ `linear featureâ€™ $y_d = \mathbf{w}_d^T\mathbf{x}$ ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ Vectorì™€ Matrix fromìœ¼ë¡œ í•œë²ˆì— ë‚˜íƒ€ë‚´ë©´</p>

\[\mathbf{y} = \mathbf{W}^T\mathbf{x}\]

<p>ë¡œ ë‘˜ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>

<p>ì´ì œ Binary Classification ë•Œì™€ ë™ì¼í•˜ê²Œ within-class covariance matrixì™€ between-class covariance matrixë¥¼ ì •ì˜í•´ì•¼ í•˜ëŠ”ë°, within-class covaraince matrixëŠ” ì´ì „ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ</p>

\[\mathbf{S}_W = \sum_{k=1}^{K}\mathbf{S}_k\]

<p>ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤. (ì—¬ê¸°ì„œ $\mathbf{S}_k$ëŠ” ê° class ë‚´ì—ì„œì˜ covariance matrix)</p>

<p>Multiple class ê°„ì˜ between-class covariance matrixì˜ ê²½ìš° total covariance matrix</p>

\[\mathbf{S}_T = \sum_{n=1}^N(\mathbf{x}_n - \mathbf{m})(\mathbf{x}_n - \mathbf{m})^T\]

<p>ì™€ covariance matrix ê°„ì˜ ê´€ê³„</p>

\[\mathbf{S}_T = \mathbf{S}_W + \mathbf{S}_B\]

<p>ë¥¼ ì´ìš©í•˜ì—¬</p>

\[\mathbf{S_B} = \sum_{k=1}^K N_k(\mathbf{m}_k - \mathbf{m})(\mathbf{m}_k - \mathbf{m})^T\]

<p>ê°€ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>Linear Features $\mathbf{y}$ì˜ ê³µê°„ì—ì„œë„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ Multiple class ê°„ì˜ $s_W$ ì™€ $s_B$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´í›„ ì—­ì‹œ between-class covarianceëŠ” í¬ê²Œí•˜ë©´ì„œ within-class covarianceë¥¼ ì‘ê²Œí•˜ê¸° ìœ„í•´</p>

\[J(W) = Tr\{s_W^{-1}s_B\} = Tr\{(\mathbf{W}\mathbf{S}_W\mathbf{W}^T)^{-1}(\mathbf{W}\mathbf{S}_B\mathbf{W}^T)\}\]

<p>ë¥¼ ìµœëŒ€í™”í•˜ëŠ” $\mathbf{W}$ë¥¼ ì°¾ìœ¼ë©´ ëœë‹¤. ì´ë•Œ, optimal $\mathbf{W}$ëŠ” $\mathbf{S}_W^{-1}\mathbf{S}_B$ì˜ Dâ€™ê°œì˜ Largest Eigenvalueì— ëŒ€ì‘í•˜ëŠ” Eigenvectorë“¤ë¡œ êµ¬í•  ìˆ˜ ìˆìŒì´ ì•Œë ¤ì ¸ìˆë‹¤. ë˜í•œ, $\mathbf{S}_B$ì˜ formulation ìƒ rankê°€ ìµœëŒ€ (K-1) ì´ê¸° ë•Œë¬¸ì—, ì˜ë¯¸ê°€ ìˆëŠ” Dâ€™ëŠ” ìµœëŒ€ (K-1)ê¹Œì§€ë§Œ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, Binary classificationì—ì„œëŠ” 1ì°¨ì›ìœ¼ë¡œì˜ projectionì´ ìµœì„ ì¸ ê²ƒì´ë‹¤.</p>

<h3 id="417-the-perceptron-algorithm">4.1.7 The Perceptron algorithm</h3>

<p>ë˜ ë‹¤ë¥¸ linear discriminant ëª¨ë¸ë¡œ Perceptron algorithmì„ ë“¤ ìˆ˜ ìˆë‹¤. ë¨¼ì € input vector $\mathbf{x}$ë¥¼ fixed nonlinear transfromationì„ í†µí•´ feature vector $\phi(\mathbf{x})$ë¡œ mappingí•œ ë’¤ (bias term í¬í•¨),</p>

\[y(\mathbf{x}) = f(\mathbf{w}^T\phi(\mathbf{x}))\]

<p>ì‹ì„ í†µí•´ $y(\mathbf{x})$ë¥¼ ê³„ì‚°í•´ì£¼ëŠ”ë°, ì´ë•Œ $f(x)$ëŠ” step functionìœ¼ë¡œ</p>

\[f(x) = \begin{cases}
  +1, &amp; x \geq 0 \\
  -1, &amp; x &lt; 0
\end{cases}\]

<p>ì˜ í˜•íƒœë¥¼ ê°€ì§„ë‹¤. Step functionì˜ ê°’ì— ë§ì¶° Perceptron algorithmì—ì„œëŠ” target valueë¥¼ $C_1$ì˜ ê²½ìš° $t = +1$, $C_2$ì˜ ê²½ìš° $t = -1$ ë¡œ ë‘”ë‹¤. ì´ì œ, target valueë¥¼ ì˜ ë§ì¶”ëŠ” $\mathbf{w}$ë¥¼ êµ¬í•´ì£¼ê¸°ë§Œ í•˜ë©´ ë˜ëŠ”ë° ì´ë¥¼ êµ¬í•˜ëŠ” analyticí•œ ë°©ë²•ì€ ì—†ê³  gradient descent ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. Gradient desecentë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” error functionì„ ì •ì˜í•´ì•¼ í•˜ëŠ”ë°, Perceptron algorithmì˜ í˜•íƒœë¥¼ ë³´ë©´, classë¥¼ ì˜ ë§ì¶˜ ê²½ìš°ì—ëŠ” $\mathbf{w}^T\phi(\mathbf{x}_n)t_n$ ê°’ì´ ì–‘ìˆ˜ë¥¼ classë¥¼ ì˜ ë§ì¶”ì§€ ëª»í•œ ê²½ìš°ëŠ” ìŒìˆ˜ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ ì´ìš©í•´ perceptron criterionì´ë¼ ë¶ˆë¦¬ëŠ” error function</p>

\[E_p(\mathbf{w}) = - \sum_{n \in M}\mathbf{w}^T\phi(\mathbf{x_n})t_n\]

<p>ì„ ì •ì˜í•˜ì—¬ (ì—¬ê¸°ì„œ $M$ì€ Classë¥¼ ë§ì¶”ì§€ ëª»í•œ observationë“¤ì„ ì˜ë¯¸í•œë‹¤) gradient descent ë°©ë²•ì„ í†µí•´ $\mathbf{w}$ë¥¼ update í•œë‹¤. ì¦‰,</p>

\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta\bigtriangledown E_p(\mathbf{w}) = \mathbf{w}^{(t)} +\eta\phi(\mathbf{x_n})t_n\]

<p>ì˜ ê³„ì‚°ì„ í†µí•´ $\mathbf{w}$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. í•´ë‹¹ ë°©ë²•ì€ perceptron convergence theoremì— ë”°ë¼ì„œ dataê°€ linearly separable í•˜ë‹¤ë©´ finite steps ì•ˆì— converge í•œë‹¤ëŠ” ê²ƒì´ ì¦ëª…ë˜ì–´ ìˆë‹¤. í•˜ì§€ë§Œ, linearly separable í•˜ì§€ ì•Šì€ ê²½ìš° perceptron algorithmì€ solutionì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ perceptron algorithm ì—¬ëŸ¬ê°œë¥¼ ì´ì–´ ë¶™ì´ëŠ” í˜•íƒœì˜ ì•Œê³ ë¦¬ì¦˜ì´ ë“±ì¥í•˜ê²Œ ëœë‹¤. ì´ë¡œì¸í•´ perceptron algorithmì„ Neural Networkì˜ ì¡°ìƒìœ¼ë¡œ ë³´ëŠ” ì‹œê°ë„ ì¡´ì¬í•œë‹¤.</p>

<h2 id="42-probabilistic-generative-model">4.2 Probabilistic Generative Model</h2>

<p>ì§€ê¸ˆê¹Œì§€ ë‹¤ë¤˜ë˜ Classification ëª¨ë¸ë“¤ì€ Discriminative approachë¡œ observationì´ ì–´ëŠ classì— ì†í•˜ëŠ”ì§€ë§Œ ì•Œë ¤ì¤€ë‹¤ëŠ” í•œê³„ê°€ ìˆë‹¤. ì´ì— ë°˜í•´ Probabilistic Generative approachëŠ” ì–´ëŠ classì— ì†í•  í™•ë¥ ì„ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ êµ‰ì¥íˆ ìœ ìš©í•œ ì ‘ê·¼ë°©ë²•ì´ë‹¤. ì´ë¥¼ ìœ„í•´ class conditional density 
<span>$p(\mathbf{x|C_k})$</span>
ì™€ class priors 
<span>$p(C_k)$</span>
ë¥¼ ë¨¼ì € êµ¬í•˜ê³ , Bayesâ€™ Theoremì„ ì´ìš©í•´ posterior 
<span>$p(C_k|\mathbf{x})$</span>
ë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì´ ì¼ë°˜ì ì´ë‹¤.</p>

<p>ë¨¼ì €, two class classification ë¬¸ì œë¥¼ ìƒê°í•´ë³´ì. $C_1$ì— ëŒ€í•œ posterior probabilityëŠ”</p>

\[\begin{aligned}
p(C_1|\mathbf{x}) &amp;= \cfrac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_1)p(C_1) + p(\mathbf{x}|C_2)p(C_2)} \\ &amp;= \cfrac{1}{1 + exp(-a)} = \sigma(a) 
\end{aligned}\]

<p>where</p>

\[a = ln\cfrac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_2)p(C_2)}\]

<p>ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ $\sigma$ëŠ” sigmoid function</p>

\[\sigma(a) = \cfrac{1}{1 + exp(-a)}\]

<p>ì„ ì˜ë¯¸í•œë‹¤.</p>

<p>ì´ë¥¼ multiple classë¡œ í™•ì¥í•˜ë©´,</p>

\[p(C_k|\mathbf{x}) = \cfrac{p(\mathbf{x}|C_k)p(C_k)}{\sum_jp(\mathbf{x}|C_j)p(C_j)} = \cfrac{exp(a_k)}{\sum_jexp(a_j)}\]

<p>where</p>

\[a_k = ln \ p(\mathbf{x}|C_k)p(C_k)\]

<p>ë¡œ êµ¬í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” softmax functionì˜ í˜•íƒœì™€ ë™ì¼í•˜ë‹¤.</p>

<h3 id="421-continuous-inputs">4.2.1 Continuous inputs</h3>

<p>ë¨¼ì € input vector $\mathbf{x}$ê°€ continuous ì¸ ê²½ìš°, íŠ¹íˆ class-conditional densityê°€ Gaussianì¸ ê²½ìš°ë¥¼ ì‚´í´ë³´ì.</p>

\[p(\mathbf{x}|C_k) = \cfrac{1}{(2\pi)^{D/2}}\cfrac{1}{|\Sigma|^{1/2}}exp \bigg\{-\cfrac{1}{2}(\mathbf{x}-\mathbf{u}_k)^T\Sigma^{-1}(\mathbf{x}-\mathbf{u}_k)\bigg\}\]

<p>ë¼ê³  í•  ë•Œ (Classê°€ ê°™ì€ covariance matrixë¥¼ ê³µìœ í•œë‹¤ê³  ê°€ì •), ìœ„ ì‹ì— ëŒ€ì…ì„ í†µí•´</p>

\[p(C_1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x} + w_0)\]

<p>where</p>

\[\mathbf{w} = \Sigma^{-1}(\mathbf{u}_1 - \mathbf{u}_2)\]

\[w_0 = -\cfrac{1}{2}\mathbf{u}_1^{T}\Sigma^{-1}\mathbf{u_1} + \cfrac{1}{2}\mathbf{u}_2^{T}\Sigma^{-1}\mathbf{u_2} + ln\cfrac{p(C_1)}{p(C_2)}\]

<p>ë¡œ ì •ë¦¬ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, posterior ë¶„í¬ê°€ $\mathbf{x}$ì— ëŒ€í•´ì„œ linear í•œ í˜•íƒœ (sigmoid í•¨ìˆ˜ ì•ˆì—ì„œ)ê°€ ëœë‹¤. ê·¸ë¦¬ê³  prior $p(C_k)$ëŠ” parallel shiftì—ë§Œ ì˜í–¥ì„ ë¯¸ì¹¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>Kê°œì˜ classê°€ ìˆëŠ” ê²½ìš°ë¡œ í™•ì¥í•˜ì—¬ë„ ë‹¤ì‹œ ëŒ€ì…ì„ í†µí•´</p>

\[a_k(\mathbf{x}) = \mathbf{w}_k\mathbf{x} + w_{k0}\]

<p>where</p>

\[\mathbf{w}_k = \Sigma^{-1}\mathbf{u}_k\]

\[w_{k0} = -\cfrac{1}{2}\mathbf{u}_k^{T}\Sigma^{-1}\mathbf{u_k} + ln \ p(C_k)\]

<p>ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—­ì‹œ $\mathbf{x}$ì— ëŒ€í•´ì„œ linearí•œ formìœ¼ë¡œ ë‚˜íƒ€ë‚¨ì„ í™•ì¸ í•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ê° classê°€ ë™ì¼í•œ covariance matrixë¥¼ ê³µìœ í•œë‹¤ëŠ” ê°€ì •ìœ¼ë¡œ ì¸í•œ ê²ƒìœ¼ë¡œ ëª¨ë‘ ë‹¤ë¥¸ covariance matrixë¥¼ ê°€ì •í•˜ë©´ quadraticí•œ í˜•íƒœë¥¼ ê°€ì§€ê²Œ ë˜ì–´ quadratic discriminant boundaryë¥¼ ê°€ì§€ê²Œ ëœë‹¤.</p>

<h3 id="422-maximum-likelihood-solution">4.2.2 Maximum likelihood solution</h3>

<p>ì´ì œ, parametric formì„ ëª¨ë‘ êµ¬í–ˆìœ¼ë¯€ë¡œ MLEë¥¼ í†µí•´ parameter ê°’ì„ estimate í•´ì•¼í•œë‹¤. Prior $p(C_1) = \pi$ ë¼ í•˜ê³  $C_1$ì— ì†í•˜ëŠ” ê²½ìš°ë¥¼ $t_n$ =1 ë¡œ ë‘ì. ê·¸ëŸ¼ ë°˜ëŒ€ë¡œ $p(C_2) = (1 - \pi)$ê°€ ë  ê²ƒì´ë©° $C_2$ì— ì†í•˜ëŠ” ê²½ìš° $t_n$ = 0ìœ¼ë¡œ ë‘ì. ê·¸ëŸ¼</p>

\[p(\mathbf{x}_n,C_1) = p(C_1)p(\mathbf{x_n}|C_1) = \pi N(\mathbf{x_n|\mathbf{u}_1,\Sigma})\]

\[p(\mathbf{x}_n,C_2) = p(C_2)p(\mathbf{x_n}|C_2) = \pi N(\mathbf{x_n|\mathbf{u}_2,\Sigma})\]

<p>ê°€ ë˜ì–´ ìµœì¢… likelihood functionì€</p>

\[p(\mathbf{t}|\pi,\mathbf{u}_1,\mathbf{u}_2,\Sigma) = \prod_{n = 1}^{N}[\pi N(\mathbf{x_n|\mathbf{u}_1,\Sigma})]^{t_n}[\pi N(\mathbf{x_n|\mathbf{u}_2,\Sigma})]^{1-t_n}\]

<p>ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤. í•´ë‹¹ì‹ì— logë¥¼ ì·¨í•œ ë’¤ ê° parameterë¡œ ë¯¸ë¶„í•˜ì—¬ ìµœì ê°’ì„ ì°¾ìœ¼ë©´,</p>

\[\hat{\pi} = \cfrac{N_1}{N_1 + N_2}\]

\[\hat{\mathbf{u}}_1 = \cfrac{1}{N_1}\sum_{n = 1}^{N}t_n\mathbf{x_n}\]

\[\hat{\mathbf{u}}_2 = \cfrac{1}{N_2}\sum_{n = 1}^{N}(1 - t_n)\mathbf{x_n}\]

\[\hat{\Sigma} = \cfrac{N_1}{N}S_1 + \cfrac{N_2}{N}S_2\]

<p>where</p>

\[S_1 = \cfrac{1}{N_1}\sum_{n \in C_1}(\mathbf{x}_n - \mathbf{u}_1)(\mathbf{x}_n - \mathbf{u}_1)^{T}\]

\[S_2 = \cfrac{1}{N_2}\sum_{n \in C_2}(\mathbf{x}_n - \mathbf{u}_2)(\mathbf{x}_n - \mathbf{u}_2)^{T}\]

<p>ë¡œ ì›í•˜ëŠ” ê°’ë“¤ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤. Multiple classì˜ ê²½ìš°ì—ë„ ë™ì¼í•œ ë°©ë²•ì„ í†µí•´ ê° parameterë¥¼ ì–´ë µì§€ ì•Šê²Œ ì°¾ì„ ìˆ˜ ìˆë‹¤.</p>

<h3 id="423-discrete-features">4.2.3 Discrete features</h3>

<p>ì´ì œ Continuous í•œ ê²½ìš°ê°€ ì•„ë‹Œ inputì´ discrete í•œ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì. ê°„ë‹¨í•˜ê²Œ binary caseë¥¼ ìƒê°í•´ë³´ë©´, D dimension input vectorëŠ” ê° $x_i \in {0,1}, i = 1,â€¦,D$ ë¡œ ì´ë£¨ì–´ì§ˆ ê²ƒì´ê³  ì „ì²´ distributionì€ ê° class ë³„ë¡œ $2^D$ê°œì˜ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ tableë¡œ í‘œí˜„ë  ê²ƒì´ë‹¤. (Summation constraint ë•Œë¬¸ì— ì‹¤ì œ independentí•œ ë³€ìˆ˜ëŠ” $(2^D - 1)$ê°œ)</p>

<p>ì—¬ê¸°ì„œ ê° feature valueê°€ $C_k$ì— ëŒ€í•´ conditionally independent í•˜ë‹¤ëŠ” naive Bayes ê°€ì •ì„ í•˜ë©´, class-conditional distributionì€</p>

\[p(\mathbf{x}|C_k) = \prod_{i = 1}^{D}u_{ki}^{x_i}(1 - u_{ki})^{1-x_i}\]

<p>ì˜ í˜•íƒœë¥¼ ê°€ì§„ë‹¤. ì´í›„, ì´ë¥¼ ìœ„ì˜ $a_k$ë¥¼ êµ¬í•˜ëŠ” ì‹ì— ëŒ€ì…í•˜ë©´,</p>

\[a_k(\mathbf{x}) = \sum_{i = 1}^{D}\{x_iln \ u_{ki} + (1-x_i) ln \ (1-u_{ki})\} + ln \ p(C_k)\]

<p>ë¡œ ë‚˜íƒ€ë‚¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. Binary caseê°€ ì•„ë‹Œ Multicategory caseì—ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‹ì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="424-exponential-family">4.2.4 Exponential Family</h3>

<p>ì‚¬ì‹¤ Continuous (Normal) í˜¹ì€ Discrete (Bernoulli)í•œ ê²½ìš° ì™¸ì—ë„ Exponential Familyì—ì„œ ì†í•˜ëŠ” ëª¨ë“  í™•ë¥  ë¶„í¬ë¥¼ class-conditional distributionìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ í˜•íƒœì˜ input vectorì— ëŒ€í•´ì„œ Generative Modelì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<p>ì¼ë°˜ì ì¸ Exponential Familyì˜ p.d.f.ëŠ”</p>

\[p(\mathbf{x}|\boldsymbol{\lambda}_k) = h(\mathbf{x})g(\boldsymbol{\lambda}_k)exp\{\boldsymbol{\lambda}_k^{T}\mathbf{u}(\mathbf{x})\}\]

<p>ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ë° (Class ë³„ë¡œ $\boldsymbol{\lambda}_k$ê°€ ë‹¤ë¥¸ ê°’ì„ ê°€ì§„ë‹¤ê³  ê°€ì •), ì´ ì¤‘ $u(\mathbf{x}) = \mathbf{x}$ ì¸ ê²½ìš°ë§Œ ìƒê°í•˜ê³  scaling parameter $s$ë¥¼ ë„ì…í•˜ë©´,</p>

\[p(\mathbf{x}|\boldsymbol{\lambda}_k, s) = \cfrac{1}{s}h\bigg(\cfrac{1}{s}\mathbf{x}\bigg)g(\boldsymbol{\lambda}_k)exp\bigg\{\cfrac{1}{s}\boldsymbol{\lambda}_k^{T}\mathbf{x}\bigg\}\]

<p>ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. í•´ë‹¹ class-conditional distributionì„ ì´ìš©í•˜ë©´ binary classificationì˜ ê²½ìš°ì—ëŠ”</p>

\[a(\mathbf{x}) = (\boldsymbol{\lambda}_1 - \boldsymbol{\lambda}_2)^{T}\mathbf{x} + ln \ g(\boldsymbol{\lambda}_1) - ln \ g(\boldsymbol{\lambda}_2) + ln \ p(C_2) - ln \ p(C_1)\]

<p>ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©° multiple classì˜ ê²½ìš°</p>

\[a_k(\mathbf{x}) = \boldsymbol{\lambda}_k^T\mathbf{x} + ln \ g(\boldsymbol{\lambda}_k) + ln \ p(C_k)\]

<p>ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ë°, ëª¨ë‘ $\mathbf{x}$ ì— ëŒ€í•´ì„œ linear í•œ í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>
:ET