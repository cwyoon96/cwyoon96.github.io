var store = [{
        "title": "PRML 스터디 Chap.2 (1)",
        "excerpt":"(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.) PRML의 Chap.2 에서는 통계학에서 그리고 머신러닝에서 많이 쓰이는 여러 확률분포에 대한 내용을 다루고 있다. 2.1 Binary Variables 먼저, Binary Variable을 설명하기 위한 확률 분포들이다. Binary Random Variable이란 결과가 0 혹은 1 과 같이 둘 중 하나로 정해지는 Variable을...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-2-1/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.2 (2)",
        "excerpt":"(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.) 이번에는 PRML 2.3절에 해당하는 Gaussian Distribution에 대한 내용이다. 정규분포 혹은 가우시안 분포로 불리는 Gaussian Distribution은 통계학 그리고 머신러닝에서 정말 중요한 위치를 가지는 분포인 만큼 책에도 많은 내용이 담겨져 있다. 특히, 책에는 수식을 통해 결과값을 유도하는 과정이 많이 담겨져...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-2-2/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.3.3",
        "excerpt":"(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.) PRML 3.3절의 주제는 Bayesian Linear Regression이다. 이전 3.1, 3.2절에서 다뤘던 Linear Regression을 베이지안의 관점에서 모델링한 결과를 보여준다. Bayesian Linear Regression은 특히 더 뒤에 Chapter에서 다룰 Gaussian Process와 깊은 관련이 있으니 열심히 공부하도록 하자. 3.3 Bayesian Linear Model 기존의...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-3-3/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.3.4",
        "excerpt":"(본 내용은 독자가 학부 확률론 및 수리통계학 지식이 있다는 가정 하에 작성되었습니다.) PRML 3.4절의 주제는 Bayesian Model Comparison이다. 3.4절에서는 굉장히 conceptual 한 내용이 나와서 한번에 이해하기 쉽지 않을 수 있지만, 3.5절에서 더 구체적 예시를 통해 설명하니 쉽게 이해가 되지 않아도 너무 걱정하지 말자. 3.4 Bayesian Model Comparison 이번 절에서는 베이지안...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-3-4/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.4.2",
        "excerpt":"4.1.6 Fisher’s discriminant for multiple classes 이제는 기존의 Binary Classification을 넘어, Fisher’s LDA를 이용해 multiple class discrimination을 어떻게 수행하는지 다룰 것이다. K(&gt;2) 개의 class가 있고, input space의 dimension이 D라고 가정하자 (D &gt; K). 이전의 Binary Classification에서는 1차원으로 데이터를 Project 했지만, 이번에는 더 확장하여 임의의 D’ 차원으로 Project하는 방법을 다룰 것이다....","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-4-2/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.4.3",
        "excerpt":"4.3 Probabilistic Discriminative Model 4.2절에서는 $p(\\mathbf{x}|C_k)$ 에 대한 분포 가정을 통해 Bayes Thm을 이용하여 posterior $p(C_k|\\mathbf{x})$ 를 구하는 과정을 설명했다. 그 결과 $p(C_k|\\mathbf{x})$ 가 $\\mathbf{x}$ 에 대한 linear form으로 나타나는 것을 확인할 수 있었다. 4.3절에서는 거기서 착안하여, $p(\\mathbf{x}|C_k)$ 에 대한 분포가정 없이 Generalized Linear Model을 이용해 Probabilistic Discriminative Model을 생성하고...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-4-3/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.5.2-3",
        "excerpt":"5.2.2 Local quadratic approximation Error function $E(\\mathbf{w})$에 대해 point $\\hat{\\mathbf{w}}$ 를 기준으로 Taylor expansion을 하면 \\[E(\\mathbf{w}) \\simeq E(\\hat{\\mathbf{w}}) + (\\mathbf{w}-\\hat{\\mathbf{w}})^{T}\\mathbf{b} + \\cfrac{1}{2}(\\mathbf{w} - \\hat{\\mathbf{w}})^{T}\\mathbf{H}(\\mathbf{w} - \\hat{\\mathbf{w}})\\] where \\[\\mathbf{b} \\equiv \\bigtriangledown E|_{\\mathbf{w} = \\hat{\\mathbf{w}}}\\] and \\[\\mathbf{H} = \\bigtriangledown\\bigtriangledown E|_{\\mathbf{w} = \\hat{\\mathbf{w}}}\\] 으로 정리된다. 여기서 gradient에 대한 local approximation을 유도하면 \\[\\bigtriangledown E...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-2-3/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.5.4",
        "excerpt":"5.4 The Hessian Matrix 지금까지는 1차 미분값 (gradient)를 구하기 위해 backpropagation을 쓰는 방법을 공부했지만, backpropagation을 이용하여 2차 미분값 \\[\\cfrac{\\partial^2 E}{\\partial w_{ji}\\partial w_{lk}}\\] 을 구할 수 도 있다. 이 때, 2차 미분 값을 모아둔 matrix를 Hessian Matrix라고 부르는데, Hessian은 다음과 같은 역할로 사용된다. 다양한 비선형 최적화 알고리즘은 2차 미분값의 특성에 기반을...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-4/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.5.7",
        "excerpt":"5.5.5 Training with transformed data Model에 transformation invariance를 유도하는 또 다른 방법으로 당연히 transformed 된 데이터를 함께 학습에 이용하는 것을 생각할 수 있다. 이번 절에서는 해당 방법이 5.5.4절에서 다뤘던 tangent propagation과 매우 긴밀하게 연결되어 있다는 것을 보일 것이다. 5.5.4절에서와 같은 transformation을 가정하고, sum-of-square error function을 사용한다고 하면 untransformed inputs를 이용한...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-7/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.6.1-2",
        "excerpt":"6. Kernel Methods 고정된 비선형 feature space mapping을 $\\phi(\\mathbf{x})$라 할때, kernel function은 \\[k(\\mathbf{x},\\mathbf{x}') = \\phi(\\mathbf{x})^{T}\\phi(\\mathbf{x}')\\] 로 정의된다. (여기서 $\\mathbf{x}’$는 $\\mathbf{x}^T$의 의미가 아니라 임의의 다른 vector를 의미한다.) 정의상 kernel function은 기본적으로 symmetric 하다는 것을 알 수 있다. Kernel function의 가장 간단한 형태로는 identity mapping을 이용한 linear kernel \\[k(\\mathbf{x},\\mathbf{x}') = \\mathbf{x}^{T}\\mathbf{x}'\\] 을...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-6-1-2/",
        "teaser": null
      },{
        "title": "PRML 스터디 Chap.6.4",
        "excerpt":"6.4.5 Gaussian processes for classification 이번 절에서는 Gaussian process를 이용하여 classification 문제를 푸는 방법을 소개한다. 기본적으로 Gaussian process는 real axis 전체에서 prediction을 하기 때문에 sigmoid function $\\sigma(x)$을 이용해 probability를 predict 할 수 있도록 해야한다. Function $a(\\mathbf{x})$에 대한 Gaussian process를 구했다고 가정하면, target variable $t$의 분포는 Bernoulli distribution \\[p(t|a) = \\sigma(a)^{t}(1...","categories": ["PRML 스터디"],
        "tags": ["Machine Learning","PRML"],
        "url": "/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-6-4/",
        "teaser": null
      }]
