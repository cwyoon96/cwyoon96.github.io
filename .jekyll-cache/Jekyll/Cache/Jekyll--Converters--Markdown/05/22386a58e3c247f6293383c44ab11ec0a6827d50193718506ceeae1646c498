I"¯!<h1 id="6-kernel-methods">6. Kernel Methods</h1>

<p>ê³ ì •ëœ ë¹„ì„ í˜• feature space mappingì„ $\phi(\mathbf{x})$ë¼ í• ë•Œ, kernel functionì€</p>

\[k(\mathbf{x},\mathbf{x}') = \phi(\mathbf{x})^{T}\phi(\mathbf{x}')\]

<p>ë¡œ ì •ì˜ëœë‹¤. (ì—¬ê¸°ì„œ $\mathbf{x}â€™$ëŠ” $\mathbf{x}^T$ì˜ ì˜ë¯¸ê°€ ì•„ë‹ˆë¼ ì„ì˜ì˜ ë‹¤ë¥¸ vectorë¥¼ ì˜ë¯¸í•œë‹¤.) ì •ì˜ìƒ kernel functionì€ ê¸°ë³¸ì ìœ¼ë¡œ symmetric í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. Kernel functionì˜ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœë¡œëŠ” identity mappingì„ ì´ìš©í•œ linear kernel</p>

\[k(\mathbf{x},\mathbf{x}') = \mathbf{x}^{T}\mathbf{x}'\]

<p>ì„ ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>

<p>ë§ì€ kernelë“¤ì€ argument ê°„ì˜ differenceaë§Œìœ¼ë¡œ ì •ì˜ë˜ëŠ” 
$k(\mathbf{x},\mathbf{x}â€™) = k(\mathbf{x} - \mathbf{x}â€™)$
ì˜ í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆëŠ”ë° ì´ë¥¼ <em>stationary kernel</em> ì´ë¼ ë¶€ë¥¸ë‹¤. ë” ë‚˜ì•„ê°€ <em>radial basis functions</em>ë¡œë„ ì•Œë ¤ì§„ <em>homogeneous</em> kernelë“¤ì€ 
$k(\mathbf{x},\mathbf{x}â€™) = k(||\mathbf{x} - \mathbf{x}â€™||)$
 ì™€ ê°™ì´ distanceì˜ magnitudeì—ë§Œ ì˜ì¡´í•˜ëŠ” í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ëŠ” kernel functionì´ 2ê°œ input ê°„ì˜ similarityë¥¼ ì¬ëŠ” functionìœ¼ë¡œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h2 id="61-dual-representations">6.1. Dual Representations</h2>

<p>ë§ì€ linear modelë“¤ì€ kernel functionìœ¼ë¡œ í‘œí˜„ë˜ëŠ” dual representationì„ ê°€ì§€ê³  ìˆë‹¤. Linear regressionì„ ì˜ˆë¡œ ë“¤ìë©´</p>

\[J(\mathbf{w}) = \cfrac{1}{2} \sum_{n=1}^{N}\{ \mathbf{w}^{T}\phi(\mathbf{x}_n) - t_n \}^2 + \cfrac{\lambda}{2}\mathbf{w}^{T}\mathbf{w}\]

<p>ì—ì„œ graientê°€ 0ì´ ë˜ëŠ” ì§€ì ì€</p>

\[\mathbf{w} = -\cfrac{1}{\lambda}\sum_{n=1}^{N}\{ \mathbf{w}^{T}\phi(\mathbf{x}_n) - t_n \}\phi(\mathbf{x}_n) = \sum_{n=1}^{N}a_n\phi(\mathbf{x}_n) = \Phi^T\mathbf{a}\]

<p>ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<p>ì´ì œ $\mathbf{w}$ë¥¼ ì›ë˜ sum-of-square ì‹ì— subtitueí•˜ë©´</p>

\[J(\mathbf{a}) = \cfrac{1}{2}\mathbf{a}^T\mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{a}^{T}\mathbf{K}\mathbf{t} + \cfrac{1}{2}\mathbf{t}^{T}\mathbf{t} + \cfrac{\lambda}{2}\mathbf{a}^{T}\mathbf{K}\mathbf{a}\]

<p>where</p>

\[\mathbf{K} = \Phi\Phi^T \ \text{while} \ K_{nm} = k(\mathbf{x}_n,\mathbf{x}_m)\]

<p>ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. (ì—¬ê¸°ì„œ $\mathbf{K}$ë¥¼ gram matrixë¼ê³  í•œë‹¤.)</p>

<p>ë‹¤ì‹œ í•´ë‹¹ ì‹ì˜ gradientë¥¼ 0ìœ¼ë¡œ ë‘ë©´ solutionì€</p>

\[\mathbf{a} = (\mathbf{K} + \lambda \mathbf{I}_N)^{-1}\mathbf{t}\]

<p>ë¡œ ì£¼ì–´ì§„ë‹¤.</p>

<p>í•´ë‹¹ solutionì„ linear regression modelì— ëŒ€ì…í•˜ì—¬ new input $\mathbf{x}$ì— ëŒ€í•œ predictionì„ í•˜ë©´</p>

\[y(\mathbf{x}) = \mathbf{w}^{T}\phi(\mathbf{x}) = \mathbf{a}^{T}\Phi\phi(\mathbf{x}) = \mathbf{k}(\mathbf{x})^{T}(\mathbf{K} + \lambda\mathbf{I}_N)^{-1}\mathbf{t}\]

<p>where</p>

\[k_n(\mathbf{x}) = k(\mathbf{x_n},\mathbf{x}) \ \text{is element of } \mathbf{k(\mathbf{x})}\]

<p>ë¡œ ì£¼ì–´ì§„ë‹¤. ë”°ë¼ì„œ least-squares problemì— ëŒ€í•œ dual formulationì€ kernel functionë§Œì„ ê°€ì§€ê³ ë„ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŠ” feature vector $\phi(\mathbf{x})$ì˜ explicit introduction ì—†ì´ë„ kernel functionì„ ì§ì ‘ì ìœ¼ë¡œ control í•˜ì—¬ ë‹µì„ ì°¾ì„ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•˜ëŠ”ë°, ì´ë¥¼ <em>kernel trick</em>ì´ë¼ í•œë‹¤.</p>

<h2 id="62-constructing-kernels">6.2. Constructing Kernels</h2>

<p>Kernel functionì„ ì§ì ‘ì ìœ¼ë¡œ constructí•˜ê¸° ìœ„í•´ì„œëŠ” í•´ë‹¹ functionì´ validí•œ kernelì¸ì§€, ì¦‰ feature space ìƒì˜ scalar productë¡œ í‘œí˜„ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼í•œë‹¤. ë‹¤í–‰íˆ ì´ë¥¼ ì§ì ‘ í™•ì¸í•  í•„ìš” ì—†ì´, Gram matrix $\mathbf{K}$ê°€ ì–´ë–¤ set ${\mathbf{x}_n}$ì„ í†µí•´ construct í•˜ë˜ì§€ poisitve semidefinite í•¨ì„ ë³´ì´ë©´ í•´ë‹¹ kernel functionì€ valid kernelì„ì´ ì•Œë ¤ì ¸ ìˆë‹¤.</p>

<p>ìƒˆë¡œìš´ kernel functionì„ constructí•˜ëŠ”ë° ìœ ìš©í•œ ì‚¬ì‹¤ ì¤‘ í•˜ë‚˜ëŠ” ê°„ë‹¨í•œ í˜•íƒœì˜ kernelì„ building blockìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ kernelì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p><img src="/assets/img/2022-03-03-prml-ìŠ¤í„°ë””-chap-6-1-2/Kernel_Techniques.png" alt="" /></p>

<p>ì˜ˆë¥¼ ë“¤ì–´ ì„ì˜ì˜ degree $M$ì˜ polynoimal kernel</p>

\[k(\mathbf{x},\mathbf{x}') = (\mathbf{x}^T \mathbf{x}' + c)^{M} \ \text{with} \ c &gt; 0\]

<p>ì—­ì‹œ ìœ„ì˜ techniqueì„ í†µí•´ valid kernelì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ë§ì´ ì‚¬ìš©ë˜ëŠ” ë˜ ë‹¤ë¥¸ kernelì€</p>

\[k(\mathbf{x},\mathbf{x}') = \text{exp}(-||\mathbf{x} - \mathbf{x}'||^2/2\sigma^2)\]

<p>ì˜ í˜•íƒœë¥¼ ê°€ì§„ kernelë¡œ â€˜Gaussian kernelâ€™ë¡œ ë¶ˆë¦°ë‹¤. Gaussian kernelì€ infinite dimensionalityë¥¼ ê°€ì§€ëŠ” feature vectorê°„ì˜ inner productì™€ ë™ì¼í•¨ì´ ì•Œë ¤ì ¸ ìˆì–´ ë§¤ìš° ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤. Gaussian kernelì€ Euclidean distanceì— êµ­í•œë˜ì§€ ì•Šê³  ë‹¤ë¥¸ nonlinear kernel $\kappa(\mathbf{x},\mathbf{x}â€™)$ì„ ì´ìš©í•˜ì—¬</p>

\[k(\mathbf{x},\mathbf{x}') = \text{exp}\bigg\{-\cfrac{1}{2\sigma^2}(\kappa(\mathbf{x},\mathbf{x}) + \kappa(\mathbf{x}',\mathbf{x}') - 2\kappa(\mathbf{x},\mathbf{x}')) \bigg\}\]

<p>ì˜ ë” ë³µì¡í•œ í˜•íƒœë¥¼ ê°€ì§ˆ ìˆ˜ë„ ìˆë‹¤.</p>

<p>Kernel methodì˜ ì¤‘ìš”í•œ contribution ì¤‘ í•˜ë‚˜ëŠ” ë‹¨ìˆœí•œ real numberê°€ ì•„ë‹Œ graph, sets, strings, text documentsì™€ ê°™ì€ symbolic inputê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $A_1,A_2$ê°€ subsetì´ë¼ê³  í•  ë•Œ,</p>

\[k(A_1,A_2) = 2^{|A_1 \cap A_2|}\]

<p>ì€ valid kernel functionìœ¼ë¡œì„œ ê·¸ ì—­í• ì„ í•  ìˆ˜ ìˆë‹¤.</p>

<p>Kernelì„ constructí•˜ëŠ” ê°•ë ¥í•œ ë°©ì‹ ì¤‘ í•˜ë‚˜ëŠ” probabilistic generative modelì—ì„œë¶€í„° ì‹œì‘í•˜ëŠ” ë°©ì‹ì´ë‹¤. Generative model $p(\mathbf{x})$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, kernel functionì„</p>

\[k(\mathbf{x},\mathbf{x}') = p(\mathbf{x})p(\mathbf{x}')\]

<p>ì²˜ëŸ¼ ì •ì˜í•˜ë©´ ì´ëŠ” valid kernel functionì´ë‹¤. ì´ë¥¼ ë‹¤ì‹œ ìœ„ì˜ techniqueì„ ì´ìš©í•˜ì—¬</p>

\[k(\mathbf{x},\mathbf{x}') = \sum_{i}p(\mathbf{x}|i)p(\mathbf{x}'|i)p(i)\]

<p>ë¡œ extendí•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” $i$ê°€ latent variableì˜ ì—­í• ì„ í•˜ëŠ” factorizableí•œ mixture distributionìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ sumì„ ë¬´í•œë²ˆ í•œë‹¤ê³  ìƒê°í•˜ë©´ continuousí•œ latent variableì— ëŒ€í•œ</p>

\[k(\mathbf{x},\mathbf{x}') = \int p(\mathbf{x}|\mathbf{z})p(\mathbf{x}'|\mathbf{z})p(\mathbf{z})d\mathbf{z}\]

<p>kernel function í˜•íƒœì„ ìƒê°í•  ìˆ˜ ìˆë‹¤.</p>

<p>Kernel functionì„ define í•˜ëŠ”ë° generative modelì„ ì‚¬ìš©í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ <em>Fisher kernel</em>ë¡œ ì•Œë ¤ì§„ ë°©ë²•ì´ë‹¤. Parametric generative model 
$p(\mathbf{x}|\boldsymbol{\theta})$
ê°€ ìˆë‹¤ê³  í–ˆì„ ë•Œ, generative modelì—ì„œ ìƒì„±ëœ input vectorë“¤ ê°„ì˜ similarityë¥¼ ì¬ëŠ” kernel functionì„ ìƒê°í•´ë³´ì. ì´ë¥¼ ìœ„í•´ <em>Fisher score</em></p>

\[\mathbf{g}(\boldsymbol{\theta},\mathbf{x}) = \bigtriangledown_{\theta} \ ln \ p(\mathbf{x}|\boldsymbol{\theta})\]

<p>ì™€ <em>Fisher information matrix</em></p>

\[\mathbf{F} = \mathbb{E}_{p(\mathbf{x}|\boldsymbol{\theta})} [\mathbf{g}(\boldsymbol{\theta},\mathbf{x})\mathbf{g}(\boldsymbol{\theta},\mathbf{x})^{T}]\]

<p>ë¥¼ í†µí•´ ì •ì˜ëœ Fisher kernel</p>

\[k(\mathbf{x},\mathbf{x}') = \mathbf{g}(\boldsymbol{\theta},\mathbf{x})^T \mathbf{F}^{-1} \mathbf{g}(\boldsymbol{\theta},\mathbf{x}')\]

<p>ë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤. Fisher kenelì€ Fisher information matrixë¡œ ì¸í•´ density modelì˜ nonlinear re-parametrization 
$\boldsymbol{\theta} \rightarrow \boldsymbol{\psi}(\boldsymbol{\theta})$
 ì— ëŒ€í•´ invariant í•˜ë‹¤ëŠ” íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.</p>

<p>ì‹¤ì œë¡œëŠ” Fisher information matrixë¥¼ ê³„ì‚°í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— sample averageë¥¼ í†µí•´</p>

\[\mathbf{F} \simeq \cfrac{1}{N} \sum_{n = 1}^{N}  \mathbf{g}(\boldsymbol{\theta},\mathbf{x}_n) \mathbf{g}(\boldsymbol{\theta},\mathbf{x}_n)^{T}\]

<p>ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ Fisher information matrix ìì²´ë¥¼ ìƒëµí•´ noninvariant í•œ kernel</p>

\[k(\mathbf{x},\mathbf{x}') = \mathbf{g}(\boldsymbol{\theta},\mathbf{x})^{T}\mathbf{g}(\boldsymbol{\theta},\mathbf{x}')\]

<p>ì„ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ ì†Œê°œí•  kernelì€ sigmoidal kernelë¡œ</p>

\[k(\mathbf{x},\mathbf{x}') = \text{tanh}(a\mathbf{x}^{T}\mathbf{x}' +b)\]

<p>ì˜ í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° Gram matrixê°€ ë³´í†µ positive semidefinite í•˜ì§€ ì•Šë‹¤. í•˜ì§€ë§Œ, í•´ë‹¹ kernelì„ í†µí•´ support vector machineì´ neural network modelì„ resemble í•œë‹¤ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ê³§ ë³´ì´ê² ì§€ë§Œ basis functionì˜ ê°œìˆ˜ë¥¼ ë¬´í•œê°œë¡œ ë³´ëƒˆì„ ë•Œ, ì ì ˆí•œ priorë¥¼ ì„¤ì •í•˜ë©´ Bayesian neural networkê°€ Gaussian processë¡œ reduce í•œë‹¤ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” kernel methodê³¼ neural network ì‚¬ì´ì— ê¹Šì€ ê´€ê³„ê°€ ìˆìŒì„ ì•”ì‹œí•œë‹¤.</p>
:ET