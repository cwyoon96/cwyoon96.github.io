<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>PRML 스터디 Chap.5.2-3 | Statistics and Machine Learning Lab by cwyoon96</title>
<meta name="description" content="5.2.2 Local quadratic approximation">


  <meta name="author" content="cwyoon96">
  
  <meta property="article:author" content="cwyoon96">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Statistics and Machine Learning Lab by cwyoon96">
<meta property="og:title" content="PRML 스터디 Chap.5.2-3">
<meta property="og:url" content="http://localhost:4000/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-2-3/">


  <meta property="og:description" content="5.2.2 Local quadratic approximation">







  <meta property="article:published_time" content="2022-02-06T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-2-3/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "cwyoon96",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});

</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Statistics and Machine Learning Lab by cwyoon96 Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Statistics and Machine Learning Lab by cwyoon96
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">cwyoon96</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>통계학 그리고 머신러닝을 공부하는 사람입니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Daejeon, South Korea</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/cwyoon96" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="PRML 스터디 Chap.5.2-3">
    <meta itemprop="description" content="5.2.2 Local quadratic approximation">
    <meta itemprop="datePublished" content="2022-02-06T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-2-3/" class="u-url" itemprop="url">PRML 스터디 Chap.5.2-3
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 분 소요
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu"><li><a href="#522-local-quadratic-approximation">5.2.2 Local quadratic approximation</a></li><li><a href="#523-use-of-gradient-information">5.2.3 Use of gradient information</a></li><li><a href="#524-gradient-descent-optimization">5.2.4 Gradient descent optimization</a></li><li><a href="#531-evaluation-of-error-function-derivatives">5.3.1 Evaluation of error-function derivatives</a></li><li><a href="#532-a-simple-example">5.3.2 A simple example</a></li><li><a href="#533-efficiency-of-backpropagation">5.3.3 Efficiency of backpropagation</a></li><li><a href="#534-the-jacobian-matrix">5.3.4 The Jacobian matrix</a></li></ul>

            </nav>
          </aside>
        
        <h3 id="522-local-quadratic-approximation">5.2.2 Local quadratic approximation</h3>

<p>Error function $E(\mathbf{w})$에 대해 point $\hat{\mathbf{w}}$ 를 기준으로 Taylor expansion을 하면</p>

\[E(\mathbf{w}) \simeq E(\hat{\mathbf{w}}) + (\mathbf{w}-\hat{\mathbf{w}})^{T}\mathbf{b} + \cfrac{1}{2}(\mathbf{w} - \hat{\mathbf{w}})^{T}\mathbf{H}(\mathbf{w} - \hat{\mathbf{w}})\]

<p>where</p>

\[\mathbf{b} \equiv \bigtriangledown E|_{\mathbf{w} = \hat{\mathbf{w}}}\]

<p>and</p>

\[\mathbf{H} = \bigtriangledown\bigtriangledown E|_{\mathbf{w} = \hat{\mathbf{w}}}\]

<p>으로 정리된다. 여기서 gradient에 대한 local approximation을 유도하면</p>

\[\bigtriangledown E \simeq \mathbf{b} + \mathbf{H}(\mathbf{w} - \hat{\mathbf{w}})\]

<p>를 구할 수 있다. 이제 error function의 minimum point <span> $\mathbf{w}^{*}$ </span>를 기준으로 생각해보자. <span> $\mathbf{w}^{*}$ </span>에서는 $\bigtriangledown E = 0$ 이므로,</p>

\[E(\mathbf{w}) = E(\mathbf{w}^*) + \cfrac{1}{2}(\mathbf{w} - \mathbf{w}^*)\mathbf{H}(\mathbf{w} - \mathbf{w}^*)\]

<p>으로 정리할 수 있다. 수식의 의미를 기하학적으로 이해하기 위해 $\mathbf{H}$에 대해 eigendecomposition을 하면</p>

\[\mathbf{H}\mathbf{u}_i = \lambda_{i}\mathbf{u}_i\]

<p>으로 쓸 수 있다. 여기서 eigenvector는 complete orthonormal set 이므로,</p>

\[(\mathbf{w} - \mathbf{w}^*) = \sum_{i}\alpha_i\mathbf{u}_i\]

<p>로 나타낼 수 있다. 이는 기존의 coordinate system을 point $\mathbf{w}^*$를 origin으로 하고 eigenvectors를 축으로 하는 coordinate system으로 transform 시킨 것으로 이해할 수 있다. 이제 error function은</p>

\[E(\mathbf{w}) = E(\mathbf{w}^*) + \cfrac{1}{2}\sum_{i}\lambda_i\alpha_i^{2}\]

<p>의 형태가 된다. 이를 그림으로 나타내면</p>

<p><img src="/assets/img/2022-02-06-prml-스터디-chap-5-2-3/Fig.5.6.png" alt="" title="Figure 5.6" /></p>

<p>의 형태로 이해할 수 있다.</p>

<p>또한, one-dimensional space에서 stationary point <span> $\mathbf{w}^*$ </span> 가 mimimum이 되려면 2차 미분 값이 양수여야 하듯이, D-dimensional space에서는 Hessian Matrix가 positive definite (eigenvalue가 모두 양수)가 되어야 한다. 그러므로 minimum point <span> $\mathbf{w}^*$ </span>에 대해 항상 이러한 geometric interpretation이 가능한 것을 알 수 있다.</p>

<h3 id="523-use-of-gradient-information">5.2.3 Use of gradient information</h3>

<p>이후 5.3절 에서는 backpropagation procedure을 통해 gradient를 빠르게 계산할 수 있음을 보일 것이다. 이번 절에서는 왜 gradient information을 사용하는 것이 error function의 minima를 찾는 데에 효율적인지 살펴보자.</p>

<p>이전 5.2.2절에서 살펴본 local quadratic approximation에 따르면, error function은 $\mathbf{b}, \ \mathbf{H}$에 depend 하는데, 해당 term들은 총 $W(W+3)/2$ 만큼의 independent elements를 가지고 있다 ($W$는 weight의 dimension). 따라서 quadratic approximation의 mimimum의 location은 $O(W^2)$ 개의 parameter를 통해야 알 수 있다는 것이며, minimum을 찾기 위해서는 $O(W^2)$ 만큼의 정보를 모아야한다는 것을 알 수 있다. 따라서 gradient information을 사용하지 않으면 function evaluation을 $O(W^2)$번 해야하는데, 각 evaluation의 cost는 $O(W)$이기 때문에 총 $O(W^3)$ 만큼의 cost가 든다는 것을 알 수 있다.</p>

<p>반면에 gradient를 사용하는 경우를 생각해보자. Gradient evaluation 한 번에 $W$ 만큼의 정보가 생기므로 evaluation을 $O(W)$ 만큼 하면 minimum을 찾을 수 있다. 이때 backpropagation을 이용하면 gradient evaluation의 cost는 $O(W)$이기 때문에 총 $O(W^2)$ 만큼의 cost 만으로 minima를 찾을 수 있다는 것을 알 수 있다. Cubic time complexity를 quadratic time complexity로 줄인 것이다.</p>

<h3 id="524-gradient-descent-optimization">5.2.4 Gradient descent optimization</h3>

<p>Gradient information을 통해 weight를 update 하는 가장 쉬운 방법은 gradient descent를 통해</p>

\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \bigtriangledown E(\mathbf{w}^{(t)})\]

<p>iterative하게 minimum을 찾아가는 것이다. 이때 전체 train set을 다 쓰게 되며, 이를 batch method라고 한다. 하지만 이런 batch method는 intuitive 하지만 성능이 좋지 않으며, 이를 개선한 conjugate gradient나 quasi-newton method 등이 개발되었다. 해당 알고리즘들은 gradient descent와 다르게 local 혹은 global minima에 다다를 때 까지 항상 error function이 decrease 한다는 특성을 가진다.</p>

<p>하지만 large data set을 이용하기 위해선 on-line version of gradient descent가 필요하다. Error function은 data point에서의 maximum likelihood에 기반하기 때문에,</p>

\[E(\mathbf{w}) = \sum_{n = 1}^{N} E_n(\mathbf{w})\]

<p>으로 나타낼 수 있다. 따라서 on-line gradient descent 혹은 stochastic gradient descent는 한번에 data point 하나를 이용하여</p>

\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \bigtriangledown E_n(\mathbf{w}^{(t)})\]

<p>weight를 update 한다. Data point의 선택은 sequential 하게 혹은 random으로 repeat 된다. 또한, gradient descent와 stochastic descent의 중간 지점격으로 일정 portion의 data point를 묶어서 계산하는 경우도 존재한다.</p>

<p>이러한 stochastic descent의 장점은</p>

<ol>
  <li>계산이 더 효율적이다.</li>
  <li>Data point 전체에 대한 stationary point와 하나 하나에 대한 stationary point는 다르기 때문에 local minima를 빠져나올 가능성이 더 높다.</li>
</ol>

<p>로 정리할 수 있다.</p>

<h2 id="53-error-backpropagation">5.3 Error Backpropagation</h2>

<p>Backpropagation이란 gradient 계산을 위해 정보를 network의 forward 그리고 다시 backward에 alternative 하게 전달하는 방법을 의미한다. 먼저 network의 training phase를 좀 더 자세하게 살펴보자. 대부분의 training algorithm은 weight의 sequential update를 통해 error function를 iterative 하게 줄여나간다. 각 iteration은 2개의 distinct한 stage로 나눌 수 있는데, 첫번째는 error function의 derivative w.r.t. weights 가 계산되는 stage이다. Backpropagation은 이러한 derivative 계산을 효율적으로 하는데 큰 도움을 주는 methodology 이다. 두번째는 해당 derivative를 이용해 weight를 update 하는 stage이다. 이를 위해서 이전에 다뤘던 gradient descent와 같은 방법을 사용할 수 있다. 이 2 stage는 서로 독립적인 관계로, 양쪽에서 쓰이는 methodology는 neural network 외에도 다른 모델에 독립적으로 사용될 수 있다.</p>

<h3 id="531-evaluation-of-error-function-derivatives">5.3.1 Evaluation of error-function derivatives</h3>

<p>이제 일반적으로 backpropagation이 어떻게 이루어지는지 살펴보자. 여기서는 전체 data point가 아닌 하나의 data point에 대한 $\bigtriangledown E_n(\mathbf{w})$ 계산만 다룰 것이다.</p>

<p>먼저 hidden layer 가 하나인 neural network를 생각해보자. Hidden layer의 unit $z_j$는 input $z_i$ (hidden layer가 많아지면 input이 아닌 그 전 hidden unit으로 생각)와 activation function $h(\cdot)$를 이용하여</p>

\[z_j = h(a_j)\]

<p>where $a_j = \sum_{i}w_{ji}z_i$</p>

<p>로 나타낼 수 있다. 여기서 $w_{ji}$는 $z_i$를 $z_j$로 연결시켜주는 weight이다. 이후 output layer은 다시 $z_j$와 link function을 이용해 output unit $z_k$를 계산할 것이다. 이렇게 neural network의 구조를 통해 input vector를 이용해 output을 계산하는 과정을 forward propagation 이라 부른다.</p>

<p>이제 weight $w_{ji}$에 대한 $E_n$의 derivative를 구하는 과정을 살펴보자. $E_n$은 $a_j$를 통해서만 $w_{ji}$에 depend 하기 때문에 chain rule에 따라</p>

\[\cfrac{\partial E_n}{\partial w_{ji}} = \cfrac{\partial E_n}{\partial a_j}\cfrac{\partial a_j}{\partial w_{ji}}\]

<p>로 계산된다. Error term</p>

\[\delta_j \equiv \cfrac{\partial E_n}{\partial a_j}\]

<p>를 정의하고,</p>

\[\cfrac{\partial a_j}{\partial w_{ji}} = z_i\]

<p>임을 이용하여</p>

\[\cfrac{\partial E_n}{\partial w_{ji}} = \delta_j z_i\]

<p>를 계산할 수 있다. 이는 weight의 output end에 있는 error $\delta$에 weight의 input end에 있는 $z$를 곱한 형태로 이전의 linear model에서도 많이 볼 수 있던 형태이다. 이제 $\delta_j$를 계산해보자.</p>

<p>다시 chain rule을 이용하면</p>

\[\delta_j \equiv \cfrac{\partial E_n}{\partial a_j} = \sum_k \cfrac{\partial E_n}{\partial a_k}\cfrac{\partial a_k}{\partial a_j}\]

<p>임을 알 수 있고, 이전 Chap.4에서 다뤘듯 canonical link function을 이용하는 경우 output unit에 대한 error $\delta_k$는</p>

\[\delta_k \equiv \cfrac{\partial E_n}{\partial a_k} = y_k - t_k\]

<p>으로 계산된다. 따라서 최종 $\delta_j$의 값은</p>

\[\delta_j = h'(a_j)\sum_k w_{kj}\delta_k\]

<p>로 나타내진다. 이는 특정 hidden layer의 error term은 다음 layer의 error term에 대한 정보를 backward 전달하여 구해질 수 있다는 의미로 왜 이 방법론이 backpropagation으로 불리는지 알 수 있는 대목이다. 이러한 방법을 통해 모든 형태의 neural network에 대한 derivative calculation을 할 수 있다.</p>

<p>Batch method에 대해서는 이렇게 data point 하나에 대해 구한 derivative를 cumulative 하게 더해 구할 수 있다.</p>

\[\cfrac{\partial E}{\partial w_{ji}} = \sum_n \cfrac{\partial E_n}{\partial w_{ji}}\]

<h3 id="532-a-simple-example">5.3.2 A simple example</h3>

<p>이제 실제 예시를 통해 5.3.1 절의 결과를 살펴보자. 동일하게 hidden layer가 하나인 형태의 neural network에서 error function은 sum-of-square error를 (link function이 identity function) activation function으로 $h(\cdot) = tanh(\cdot)$을 사용한다고 가정하자.</p>

<p>이때</p>

\[h'(a) = 1 - h(a)^2 \\
\delta_k = y_k - t_k\]

<p>임을 이용하여</p>

\[\delta_j = (1 - z_j^2)\sum_{k= 1}^{K}w_{kj}\delta_k\]

<p>를 계산할 수 있고, 최종적으로</p>

\[\cfrac{\partial E_n}{\partial w_{ji}} = \delta_j z_i, \ \ \ \ \cfrac{\partial E_n}{\partial w_{kj}} = \delta_k z_j\]

<p>임을 알 수 있다.</p>

<h3 id="533-efficiency-of-backpropagation">5.3.3 Efficiency of backpropagation</h3>

<p>Backpropagation의 가장 큰 이점 중 하나는 computational efficiency 이다. Error function의 single evaluation은 $O(W)$ 만큼의 time complexity를 가진다.</p>

<p>Backpropagation을 대체하는 방법으로 finite differences를 사용할 수 있는데, 이는 derivative를 approximate 하는 방법으로</p>

\[\cfrac{\partial E_n}{\partial w_{ji}} = \cfrac{E_n(w_{ji} + \epsilon) - E_n(w_{ji})}{\epsilon} + O(\epsilon)\]

<p>혹은 symmetrical central differences (더 정확하지만 computational step이 2배)</p>

\[\cfrac{\partial E_n}{\partial w_{ji}} = \cfrac{E_n(w_{ji} + \epsilon) - E_n(w_{ji}- \epsilon)}{2\epsilon} + O(\epsilon^2)\]

<p>를 이용할 수 있다. 하지만 finite difference는 forward propagation을 하는데 $O(W)$ step, 그리고 weight 하나 하나에 대해서 approximate를 해야하기 때문에 총 $O(W^2)$ time complexity를 가지기 때문에 back propagation의 $O(W)$에 비해 훨씬 많은 시간이 걸린다는 것을 알 수 있다. (하지만 finite difference 방법은 backpropagation이 정확하게 implement 되었는지 확인하기 위한 용도로 자주 사용된다.)</p>

<h3 id="534-the-jacobian-matrix">5.3.4 The Jacobian matrix</h3>

<p>Backpropagationd은 weight에 대한 derivative 외에도 다른 derivative를 구하는 데에도 사용될 수 있다. 이번 절에서는 output의 input에 대한 미분으로 이루어진 Jacobian matrix</p>

\[J_{ki} \equiv \cfrac{\partial y_k}{\partial x_i}\]

<p>를 다룰 것이다. Jacobian matrix는 Figure 5.8 처럼 여러 개의 module로 이루어진 system에서 유용하게 사용된다.</p>

<p><img src="/assets/img/2022-02-06-prml-스터디-chap-5-2-3/Fig%205.8.png" alt="" /></p>

<p>Figure 5.8에서의 paramter $w$에 대해 error function E를 minimize 한다고 생각해보자. 그럼 derivative는</p>

\[\cfrac{\partial E}{\partial w} = \sum_{k,j}\cfrac{\partial E}{\partial y_k}\cfrac{\partial y_k}{\partial z_j}\cfrac{\partial z_j}{\partial w}\]

<p>로 구할 수 있으며 Jacobian은 중간 term으로 나타난다.</p>

<p>Jacobian은 input variable의 change에 대한 output variable의 local sensitivity를 의미하기 때문에 어떤 known error $\bigtriangleup x_i$의 output에 대한 contribution을 계산하는 데에도 사용된다.</p>

\[\bigtriangleup y_k \simeq \sum_i \cfrac{\partial y_k}{\partial x_i} \bigtriangleup x_i\]

<p>기본적으로 network mapping이 nonlinear 하기 때문에 Jacobian matrix의 element는 constant 값이 아닌 input에 depend 하게 된다. 따라서 위 식은 input의 작은 변화에만 유의미하며, 새로운 input vector에 대해서는 새로 re-evaluated 되어야 한다.</p>

<p>Jacobian matrix는 이전에 weight에 대한 derivative를 backpropagation을 통해 구했던 것 처럼 backpropagation을 통해 구할 수 있다.</p>

\[J_{ki} = \cfrac{\partial y_k}{\partial x_i} = \sum_j\cfrac{\partial y_k}{\partial a_j}\cfrac{\partial a_j}{\partial x_i} = \sum_j w_{ji} \cfrac{\partial y_k}{\partial a_j}\]

<p>이제 $\cfrac{\partial y_k}{\partial a_j}$ 에 대한 backpropagation을 계산하면,</p>

\[\cfrac{\partial y_k}{\partial a_j} = \sum_l \cfrac{\partial y_k}{\partial a_l} \cfrac{\partial a_l}{\partial a_j} = h'(a_j) \sum_l w_{lj} \cfrac{\partial y_k}{\partial a_l}\]

<p>가 된다. 이때, link function으로 sigmoid를 사용하면</p>

\[\cfrac{\partial y_k}{\partial a_j} = \delta_{kj}\sigma '(a_j)\]

<p>softmax를 사용하면</p>

\[\cfrac{\partial y_k}{\partial a_j} = \delta_{kj}y_k - y_ky_j\]

<p>로 계산할 수 있다.</p>

<p>Jacobian matrix에 대한 backpropagation implementation도 numerical differentiation을 통해 제대로 implement 되었는지 check 할 수 있다.</p>

\[\cfrac{\partial y_k}{\partial x_i} = \cfrac{y_k(x_i + \epsilon) - y_k(x_i - \epsilon)}{2\epsilon} + O(\epsilon^2)\]

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#machine-learning" class="page__taxonomy-item p-category" rel="tag">Machine Learning</a><span class="sep">, </span>
    
      <a href="/tags/#prml" class="page__taxonomy-item p-category" rel="tag">PRML</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#prml-%EC%8A%A4%ED%84%B0%EB%94%94" class="page__taxonomy-item p-category" rel="tag">PRML 스터디</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2022-02-06T00:00:00+09:00">February 6, 2022</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=PRML+%EC%8A%A4%ED%84%B0%EB%94%94+Chap.5.2-3%20http%3A%2F%2Flocalhost%3A4000%2Fprml%2520%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594%2Fprml-%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594-chap-5-2-3%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fprml%2520%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594%2Fprml-%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594-chap-5-2-3%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fprml%2520%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594%2Fprml-%25EC%258A%25A4%25ED%2584%25B0%25EB%2594%2594-chap-5-2-3%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-4-3/" class="pagination--pager" title="PRML 스터디 Chap.4.3
">이전</a>
    
    
      <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-4/" class="pagination--pager" title="PRML 스터디 Chap.5.4
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-6-4/" rel="permalink">PRML 스터디 Chap.6.4
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">6.4.5 Gaussian processes for classification
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-6-1-2/" rel="permalink">PRML 스터디 Chap.6.1-2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">6. Kernel Methods
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-7/" rel="permalink">PRML 스터디 Chap.5.7
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">5.5.5 Training with transformed data
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/prml%20%EC%8A%A4%ED%84%B0%EB%94%94/prml-%EC%8A%A4%ED%84%B0%EB%94%94-chap-5-4/" rel="permalink">PRML 스터디 Chap.5.4
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">5.4 The Hessian Matrix
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 cwyoon96. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>









<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'] ],
		processEscapes: true
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
});
</script>


  </body>
</html>
