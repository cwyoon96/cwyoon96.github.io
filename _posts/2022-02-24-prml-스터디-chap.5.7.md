---
title:  "PRML 스터디 Chap.5.7"

categories:
  - PRML 스터디
tags:
  - [Machine Learning, PRML]
classes: wide
toc: true
toc_sticky: true
 
date: 2022-02-24
---

### 5.5.5 Training with transformed data

Model에 transformation invariance를 유도하는 또 다른 방법으로 당연히 transformed 된 데이터를 함께 학습에 이용하는 것을 생각할 수 있다. 이번 절에서는 해당 방법이 5.5.4절에서 다뤘던 tangent propagation과 매우 긴밀하게 연결되어 있다는 것을 보일 것이다.

5.5.4절에서와 같은 transformation을 가정하고, sum-of-square error function을 사용한다고 하면 untransformed inputs를 이용한 이론적인 error function은

$$
E = \cfrac{1}{2}\int \int \{y(\mathbf{x})-t\}^{2} p(t|\mathbf{x})p(\mathbf{x})d\mathbf{x}dt
$$

로 주어진다. (single output 가정)

이제 $\xi$가 distribution $p(\xi)$ (zero mean with samll variance)에서 draw 되었다고 가정하면, 확장된 error function은

$$
\tilde{E} = \cfrac{1}{2} \int \int \int \{y(\mathbf{s}(\mathbf{x},\xi)) - t\}^2 p(t|\mathbf{x})p(\mathbf{x})p(\xi)d\mathbf{x}dtd\xi
$$

로 쓸 수 있다.

이때, tranformation function $\mathbf{s}$에 대한 Taylor expansion을 통해

$$
\begin{aligned}
\mathbf{s}(\mathbf{x},\xi) &= \mathbf{s}(\mathbf{x},0) + \left. \xi \cfrac{\partial}{\partial \xi} \mathbf{s}(\mathbf{x},\xi)\right|_{\xi = 0} + \left. \cfrac{\xi^2}{2} \cfrac{\partial^2}{\partial \xi^2} \mathbf{s}(\mathbf{x},\xi)\right|_{\xi = 0} + O(\xi^3) \\
&= \mathbf{x} + \xi \boldsymbol{\tau} + \cfrac{1}{2}\xi^2\boldsymbol{\tau'} + O(\xi^3)
\end{aligned}
$$

로 쓸 수 있어

$$
y(\mathbf{s}(\mathbf{x},\xi)) = y(\mathbf{x}) + \xi \boldsymbol{\tau}^{T} \bigtriangledown y(\mathbf{x}) + \cfrac{\xi^2}{2}\bigg[ (\boldsymbol{\tau'})^T \bigtriangledown y(\mathbf{x}) + \boldsymbol{\tau}^T \bigtriangledown \bigtriangledown y(\mathbf{x}) \boldsymbol{\tau} \bigg] + O(\xi^3)
$$

로 model function을 쓸 수 있게 된다. 이제 이를 error function에 plug-in 하면

$$
\begin{aligned}

\tilde{E} &= \cfrac{1}{2} \int \int \{ y(\mathbf{x}) - t \}^2p(t|\mathbf{x})p(\mathbf{x})d\mathbf{x}dt \\
&+ \mathbb{E}[\xi] \int \int \{ y(\mathbf{x}) - t \} \boldsymbol{\tau}^T \bigtriangledown y(\mathbf{x})p(t|\mathbf{x})p(\mathbf{x})d\mathbf{x}dt \\
&+ \mathbb{E}[\xi^2] \int \int \bigg[ \{y(\mathbf{x}) - t\}\cfrac{1}{2} \big\{ (\boldsymbol{\tau'})^T \bigtriangledown y(\mathbf{x}) + \boldsymbol{\tau}^T \bigtriangledown \bigtriangledown y(\mathbf{x}) \boldsymbol{\tau} \big\} + \\ &(\boldsymbol{\tau}^T \bigtriangledown y(\mathbf{x}))^2 \bigg]p(t|\mathbf{x})p(\mathbf{x})d\mathbf{x}dt + O(\xi^3)

\end{aligned}
$$

로 주어진다. 이때, 가정에 따라 $\mathbb{E}[\xi] = 0$ 임을 이용하고 $\mathbb{E}[\xi^2]$를 $\lambda$로 denote 하면 5.5.4절에서 보았던

$$

\tilde{E} = E + \lambda \Omega \ \ \ \ \ \ \ \ \ - (*)

$$

where

$$

\begin{aligned}

\Omega = &\int \bigg[ \{y(\mathbf{x}) - \mathbb{E}[t|\mathbf{x}]\}\cfrac{1}{2} \big\{ (\boldsymbol{\tau'})^T \bigtriangledown y(\mathbf{x}) + \boldsymbol{\tau}^T \bigtriangledown \bigtriangledown y(\mathbf{x}) \boldsymbol{\tau} \big\} \\ &+ (\boldsymbol{\tau}^T \bigtriangledown y(\mathbf{x}))^2 \bigg]p(\mathbf{x})d\mathbf{x}

\end{aligned}

$$

의 형태가 됨을 알 수 있다. ($t$로 적분하고 $O(\xi^3)$ omit)

앞서 1절에서 sum-of-square error를 최소한으로 하는 것은 $\mathbb{E}[t|\mathbf{x}]$ 임을 밝혔다. 이에 위 (\*)식으로부터 reuglarized error를 최소한으로 하는 network function은

$$
y(\mathbf{x}) = \mathbb{E}[t|\mathbf{x}] + O(\xi)
$$

임을 알 수 있고, $\xi$의 최고차항을 기준으로 regularizer의 first term이 사라져

$$
\Omega = \cfrac{1}{2} \int (\boldsymbol{\tau}^T \bigtriangledown y(\mathbf{x}))^2 p(\mathbf{x})d\mathbf{x}
$$

로 주어져 5.5.4절에서 보았던 tangent propagation regularizer와 완벽히 일치함을 알 수 있다.

## 5.7 Bayesian Neural Networks

