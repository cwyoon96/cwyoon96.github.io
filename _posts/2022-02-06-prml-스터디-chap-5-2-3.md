---
title:  "PRML 스터디 Chap.5.2-3"

categories:
  - PRML 스터디
tags:
  - [Machine Learning, PRML]
classes: wide
toc: true
toc_sticky: true
 
date: 2022-02-06
---

### 5.2.2 Local quadratic approximation

Error function $E(\mathbf{w})$에 대해 point $\hat{\mathbf{w}}$ 를 기준으로 Taylor expansion을 하면

$$
E(\mathbf{w}) \simeq E(\hat{\mathbf{w}}) + (\mathbf{w}-\hat{\mathbf{w}})^{T}\mathbf{b} + \cfrac{1}{2}(\mathbf{w} - \hat{\mathbf{w}})^{T}\mathbf{H}(\mathbf{w} - \hat{\mathbf{w}})
$$

where 

$$
\mathbf{b} \equiv \bigtriangledown E|_{\mathbf{w} = \hat{\mathbf{w}}}
$$

and

$$
\mathbf{H} = \bigtriangledown\bigtriangledown E|_{\mathbf{w} = \hat{\mathbf{w}}}
$$

으로 정리된다. 여기서 gradient에 대한 local approximation을 유도하면

$$
\bigtriangledown E \simeq \mathbf{b} + \mathbf{H}(\mathbf{w} - \hat{\mathbf{w}})
$$ 

를 구할 수 있다. 이제 error function의 minimum point <span> $\mathbf{w}^{*}$ </span>를 기준으로 생각해보자. <span> $\mathbf{w}^{*}$ </span>에서는 $\bigtriangledown E = 0$ 이므로,

$$
E(\mathbf{w}) = E(\mathbf{w}^*) + \cfrac{1}{2}(\mathbf{w} - \mathbf{w}^*)\mathbf{H}(\mathbf{w} - \mathbf{w}^*)
$$ 

으로 정리할 수 있다. 수식의 의미를 기하학적으로 이해하기 위해 $\mathbf{H}$에 대해 eigendecomposition을 하면

$$
\mathbf{H}\mathbf{u}_i = \lambda_{i}\mathbf{u}_i
$$

으로 쓸 수 있다. 여기서 eigenvector는 complete orthonormal set 이므로,

$$
(\mathbf{w} - \mathbf{w}^*) = \sum_{i}\alpha_i\mathbf{u}_i
$$ 

로 나타낼 수 있다. 이는 기존의 coordinate system을 point $\mathbf{w}^*$를 origin으로 하고 eigenvectors를 축으로 하는 coordinate system으로 transform 시킨 것으로 이해할 수 있다. 이제 error function은

$$
E(\mathbf{w}) = E(\mathbf{w}^*) + \cfrac{1}{2}\sum_{i}\lambda_i\alpha_i^{2}
$$

의 형태가 된다. 이를 그림으로 나타내면

![](/assets/img/2022-02-06-prml-스터디-chap-5-2-3/Fig.5.6.png "Figure 5.6")

의 형태로 이해할 수 있다.

또한, one-dimensional space에서 stationary point <span> $\mathbf{w}^*$ </span> 가 mimimum이 되려면 2차 미분 값이 양수여야 하듯이, D-dimensional space에서는 Hessian Matrix가 positive definite (eigenvalue가 모두 양수)가 되어야 한다. 그러므로 minimum point <span> $\mathbf{w}^*$ </span>에 대해 항상 이러한 geometric interpretation이 가능한 것을 알 수 있다.

### 5.2.3 Use of gradient information

이후 5.3절 에서는 backpropagation procedure을 통해 gradient를 빠르게 계산할 수 있음을 보일 것이다. 이번 절에서는 왜 gradient information을 사용하는 것이 error function의 minima를 찾는 데에 효율적인지 살펴보자.

이전 5.2.2절에서 살펴본 local quadratic approximation에 따르면, error function은 $\mathbf{b}, \ \mathbf{H}$에 depend 하는데, 해당 term들은 총 $W(W+3)/2$ 만큼의 independent elements를 가지고 있다 ($W$는 weight의 dimension). 따라서 quadratic approximation의 mimimum의 location은 $O(W^2)$ 개의 parameter를 통해야 알 수 있다는 것이며, minimum을 찾기 위해서는 $O(W^2)$ 만큼의 정보를 모아야한다는 것을 알 수 있다. 따라서 gradient information을 사용하지 않으면 function evaluation을 $O(W^2)$번 해야하는데, 각 evaluation의 cost는 $O(W)$이기 때문에 총 $O(W^3)$ 만큼의 cost가 든다는 것을 알 수 있다.

반면에 gradient를 사용하는 경우를 생각해보자. Gradient evaluation 한 번에 $W$ 만큼의 정보가 생기므로 evaluation을 $O(W)$ 만큼 하면 minimum을 찾을 수 있다. 이때 backpropagation을 이용하면 gradient evaluation의 cost는 $O(W)$이기 때문에 총 $O(W^2)$ 만큼의 cost 만으로 minima를 찾을 수 있다는 것을 알 수 있다. Cubic time complexity를 quadratic time complexity로 줄인 것이다.

### 5.2.4 Gradient descent optimization

Gradient information을 통해 weight를 update 하는 가장 쉬운 방법은 gradient descent를 통해

$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \bigtriangledown E(\mathbf{w}^{(t)})
$$

iterative하게 minimum을 찾아가는 것이다. 이때 전체 train set을 다 쓰게 되며, 이를 batch method라고 한다. 하지만 이런 batch method는 intuitive 하지만 성능이 좋지 않으며, 이를 개선한 conjugate gradient나 quasi-newton method 등이 개발되었다. 해당 알고리즘들은 gradient descent와 다르게 local 혹은 global minima에 다다를 때 까지 항상 error function이 decrease 한다는 특성을 가진다.

하지만 large data set을 이용하기 위해선 on-line version of gradient descent가 필요하다. Error function은 data point에서의 maximum likelihood에 기반하기 때문에,

$$
E(\mathbf{w}) = \sum_{n = 1}^{N} E_n(\mathbf{w})
$$

으로 나타낼 수 있다. 따라서 on-line gradient descent 혹은 stochastic gradient descent는 한번에 data point 하나를 이용하여

$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \bigtriangledown E_n(\mathbf{w}^{(t)})
$$ 

weight를 update 한다. Data point의 선택은 sequential 하게 혹은 random으로 repeat 된다. 또한, gradient descent와 stochastic descent의 중간 지점격으로 일정 portion의 data point를 묶어서 계산하는 경우도 존재한다.

이러한 stochastic descent의 장점은

1.  계산이 더 효율적이다.
2.  Data point 전체에 대한 stationary point와 하나 하나에 대한 stationary point는 다르기 때문에 local minima를 빠져나올 가능성이 더 높다.

로 정리할 수 있다.

## 5.3 Error Backpropagation

Backpropagation이란 gradient 계산을 위해 정보를 network의 forward 그리고 다시 backward에 alternative 하게 전달하는 방법을 의미한다. 먼저 network의 training phase를 좀 더 자세하게 살펴보자. 대부분의 training algorithm은 weight의 sequential update를 통해 error function를 iterative 하게 줄여나간다. 각 iteration은 2개의 distinct한 stage로 나눌 수 있는데, 첫번째는 error function의 derivative w.r.t. weights 가 계산되는 stage이다. Backpropagation은 이러한 derivative 계산을 효율적으로 하는데 큰 도움을 주는 methodology 이다. 두번째는 해당 derivative를 이용해 weight를 update 하는 stage이다. 이를 위해서 이전에 다뤘던 gradient descent와 같은 방법을 사용할 수 있다. 이 2 stage는 서로 독립적인 관계로, 양쪽에서 쓰이는 methodology는 neural network 외에도 다른 모델에 독립적으로 사용될 수 있다.

### 5.3.1 Evaluation of error-function derivatives

이제 일반적으로 backpropagation이 어떻게 이루어지는지 살펴보자. 여기서는 전체 data point가 아닌 하나의 data point에 대한 $\bigtriangledown E_n(\mathbf{w})$ 계산만 다룰 것이다.

먼저 hidden layer 가 하나인 neural network를 생각해보자. Hidden layer의 unit $z_j$는 input $z_i$ (hidden layer가 많아지면 input이 아닌 그 전 hidden unit으로 생각)와 activation function $h(\cdot)$를 이용하여

$$
z_j = h(a_j)
$$ 

where $a_j = \sum_{i}w_{ji}z_i$

로 나타낼 수 있다. 여기서 $w_{ji}$는 $z_i$를 $z_j$로 연결시켜주는 weight이다. 이후 output layer은 다시 $z_j$와 link function을 이용해 output unit $z_k$를 계산할 것이다. 이렇게 neural network의 구조를 통해 input vector를 이용해 output을 계산하는 과정을 forward propagation 이라 부른다.

이제 weight $w_{ji}$에 대한 $E_n$의 derivative를 구하는 과정을 살펴보자. $E_n$은 $a_j$를 통해서만 $w_{ji}$에 depend 하기 때문에 chain rule에 따라

$$
\cfrac{\partial E_n}{\partial w_{ji}} = \cfrac{\partial E_n}{\partial a_j}\cfrac{\partial a_j}{\partial w_{ji}}
$$

로 계산된다. Error term

$$
\delta_j \equiv \cfrac{\partial E_n}{\partial a_j}
$$

를 정의하고,

$$
\cfrac{\partial a_j}{\partial w_{ji}}
$$

임을 이용하여

$$
\cfrac{\partial E_n}{\partial w_{ji}} = \delta_j z_i
$$

를 계산할 수 있다. 이는 weight의 output end에 있는 error $\delta$에 weight의 input end에 있는 $z$를 곱한 형태로 이전의 linear model에서도 많이 볼 수 있던 형태이다. 이제 $\delta_j$를 계산해보자.

다시 chain rule을 이용하면

$$
\delta_j \equiv \cfrac{\partial E_n}{\partial a_j} = \sum_k \cfrac{\partial E_n}{a_k}\cfrac{\partial a_k}{\partial w_kj}
$$

임을 알 수 있고, 이전 Chap.4에서 다뤘듯 canonical link function을 이용하는 경우 output unit에 대한 error $\delta_k$는

$$
\delta_k \equiv \cfrac{\partial E_n}{\partial a_k} = y_k - t_k
$$ 

으로 계산된다. 따라서 최종 $\delta_j$의 값은

$$
\delta_j = h'(a_j)\sum_k w_{kj}\delta_k
$$

로 나타내진다. 이는 특정 hidden layer의 error term은 다음 layer의 error term에 대한 정보를 backward 전달하여 구해질 수 있다는 의미로 왜 이 방법론이 backpropagation으로 불리는지 알 수 있는 대목이다. 이러한 방법을 통해 모든 형태의 neural network에 대한 derivative calculation을 할 수 있다.

Batch method에 대해서는 이렇게 data point 하나에 대해 구한 derivative를 cumulative 하게 더해 구할 수 있다.

$$
\cfrac{\partial E}{\partial w_{ji}} = \sum_n \cfrac{\partial E_n}{\partial w_{ji}}
$$

### 5.3.2 A simple example

이제 실제 예시를 통해 5.3.1 절의 결과를 살펴보자. 동일하게 hidden layer가 하나인 형태의 neural network에서 error function은 sum-of-square error를 (link function이 identity function) activation function으로 $h(\cdot) = tanh(\cdot)$을 사용한다고 가정하자.

이때

$$
h'(a) = 1 - h(a)^2 \\
\delta_k = y_k - t_k
$$ 

임을 이용하여

$$
\delta_j = (1 - z_j^2)\sum_{k= 1}^{K}w_{kj}\delta_k
$$

를 계산할 수 있고, 최종적으로

$$
\cfrac{\partial E_n}{\partial w_{ji}} = \delta_j z_i, \ \ \ \ \cfrac{\partial E_n}{\partial w_{kj}} = \delta_k z_j 
$$

임을 알 수 있다.

### 5.3.3 Efficiency of backpropagation

Backpropagation의 가장 큰 이점 중 하나는 computational efficiency 이다. Error function의 single evaluation은 $O(W)$ 만큼의 time complexity를 가진다.

Backpropagation을 대체하는 방법으로 finite differences를 사용할 수 있는데, 이는 derivative를 approximate 하는 방법으로

$$
\cfrac{\partial E_n}{\partial w_{ji}} = \cfrac{E_n(w_{ji} + \epsilon) - E_n(w_{ji})}{\epsilon} + O(\epsilon)
$$

혹은 symmetrical central differences (더 정확하지만 computational step이 2배)

$$
\cfrac{\partial E_n}{\partial w_{ji}} = \cfrac{E_n(w_{ji} + \epsilon) - E_n(w_{ji}- \epsilon)}{2\epsilon} + O(\epsilon^2)
$$

를 이용할 수 있다. 하지만 finite difference는 forward propagation을 하는데 $O(W)$ step, 그리고 weight 하나 하나에 대해서 approximate를 해야하기 때문에 총 $O(W^2)$ time complexity를 가지기 때문에 back propagation의 $O(W)$에 비해 훨씬 많은 시간이 걸린다는 것을 알 수 있다. (하지만 finite difference 방법은 backpropagation이 정확하게 implement 되었는지 확인하기 위한 용도로 자주 사용된다.)

### 5.3.4 The Jacobian matrix

Backpropagationd은 weight에 대한 derivative 외에도 다른 derivative를 구하는 데에도 사용될 수 있다. 이번 절에서는 output의 input에 대한 미분으로 이루어진 Jacobian matrix

$$
J_{ki} \equiv \cfrac{\partial y_k}{\partial x_i}
$$

를 다룰 것이다. Jacobian matrix는 Figure 5.8 처럼 여러 개의 module로 이루어진 system에서 유용하게 사용된다.

![](/assets/img/2022-02-06-prml-스터디-chap-5-2-3/Fig%205.8.png)

Figure 5.8에서의 paramter $w$에 대해 error function E를 minimize 한다고 생각해보자. 그럼 derivative는

$$
\cfrac{\partial E}{\partial w} = \sum_{k,j}\cfrac{\partial E}{\partial y_k}\cfrac{\partial y_k}{\partial z_j}\cfrac{\partial z_j}{\partial w}
$$

로 구할 수 있으며 Jacobian은 중간 term으로 나타난다.

Jacobian은 input variable의 change에 대한 output variable의 local sensitivity를 의미하기 때문에 어떤 known error $\bigtriangleup x_i$의 output에 대한 contribution을 계산하는 데에도 사용된다.

$$
\bigtriangleup y_k \simeq \sum_i \cfrac{\partial y_k}{\partial x_i} \bigtriangleup x_i
$$ 

기본적으로 network mapping이 nonlinear 하기 때문에 Jacobian matrix의 element는 constant 값이 아닌 input에 depend 하게 된다. 따라서 위 식은 input의 작은 변화에만 유의미하며, 새로운 input vector에 대해서는 새로 re-evaluated 되어야 한다.

Jacobian matrix는 이전에 weight에 대한 derivative를 backpropagation을 통해 구했던 것 처럼 backpropagation을 통해 구할 수 있다.

$$
J_{ki} = \cfrac{\partial y_k}{\partial x_i} = \sum_j\cfrac{\partial y_k}{\partial a_j}\cfrac{\partial a_j}{\partial x_i} = \sum_j w_{ji} \cfrac{\partial y_k}{\partial a_j}
$$ 

이제 $\cfrac{\partial y_k}{\partial a_j}$ 에 대한 backpropagation을 계산하면,

$$
\cfrac{\partial y_k}{\partial a_j} = \sum_l \cfrac{\partial y_k}{\partial a_l} \cfrac{\partial a_l}{\partial a_j} = h'(a_j) \sum_l w_{lj} \cfrac{\partial y_k}{\partial a_l}
$$

가 된다. 이때, link function으로 sigmoid를 사용하면

$$
\cfrac{\partial y_k}{\partial a_j} = \delta_{kj}\sigma '(a_j)
$$

softmax를 사용하면

$$
\cfrac{\partial y_k}{\partial a_j} = \delta_{kj}y_k - y_ky_j
$$

로 계산할 수 있다.

Jacobian matrix에 대한 backpropagation implementation도 numerical differentiation을 통해 제대로 implement 되었는지 check 할 수 있다.

$$
\cfrac{\partial y_k}{\partial x_i} = \cfrac{y_k(x_i + \epsilon) - y_k(x_i - \epsilon)}{2\epsilon} + O(\epsilon^2)
$$
